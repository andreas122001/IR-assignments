{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs, string\n",
    "\n",
    "with codecs.open(\"common-english-words.txt\", \"r\", \"utf-8\") as f:\n",
    "    common_words = f.read().split(\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intelligent behavior people product mind but mind itself more human brain does'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and clean the text\n",
    "text = \"Intelligent behavior in people is a product of the mind. But the mind itself is more like what the human brain does.\"\n",
    "def clean(text):\n",
    "    # Remove stop words\n",
    "    text = \" \".join(list(filter(lambda x: x not in common_words, text.split(\" \"))))\n",
    "    # Remove punctutation\n",
    "    text = \"\".join(list(filter(lambda x: x not in string.punctuation+\"\\n\\r\\t\\0\", text)))\n",
    "    # To lower case\n",
    "    text = text.lower()\n",
    "    return text\n",
    "text = clean(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'itself': (1, 1),\n",
       " 'more': (1, 1),\n",
       " 'but': (1, 1),\n",
       " 'does': (1, 1),\n",
       " 'brain': (1, 1),\n",
       " 'intelligent': (1, 1),\n",
       " 'people': (1, 1),\n",
       " 'human': (1, 1),\n",
       " 'behavior': (1, 1),\n",
       " 'product': (1, 1),\n",
       " 'mind': (1, 2)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create inverted index\n",
    "vocab = dict([(key, (1, text.count(key))) for key in set(text.split(\" \"))])\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'itself': {1: 1},\n",
       " 'more': {1: 1},\n",
       " 'but': {1: 1},\n",
       " 'does': {1: 1},\n",
       " 'brain': {1: 1},\n",
       " 'intelligent': {1: 1},\n",
       " 'people': {1: 1},\n",
       " 'human': {1: 1},\n",
       " 'behavior': {1: 1},\n",
       " 'product': {1: 1},\n",
       " 'mind': {1: 2}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a (normal) inverted index\n",
    "# For one document this is just a frequency list\n",
    "def gen_idx(corpus):\n",
    "    # Initiate the index as a dict('term', dict('doc', num_occ))\n",
    "    idx_list = dict([(key, {}) for key in set(\" \".join(corpus).split(\" \"))])\n",
    "    for doc_idx, doc in enumerate(corpus, 1):\n",
    "        # Increment number of occurrences for each occurrence\n",
    "        for term in doc.split(\" \"):\n",
    "            if doc_idx not in idx_list[term].keys():\n",
    "                idx_list[term][doc_idx] = 0\n",
    "            idx_list[term][doc_idx] += 1\n",
    "    return idx_list\n",
    "gen_idx([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_idx_block(corpus, block_size=3):\n",
    "    # Initiate the index as a dict('term', dict('doc', [block_ids]))\n",
    "    idx_list = dict([(key, {}) for key in set(\" \".join(corpus).split(\" \"))])\n",
    "    corpus_blocks = []\n",
    "    for doc_idx, doc in enumerate(corpus, 1):\n",
    "        # Generate blocks\n",
    "        blocks = [doc.split(\" \")[block_size*i:block_size*i+block_size] for i in range(len(doc.split(\" \"))//block_size+1)]\n",
    "        blocks = list(filter(lambda x: len(x)>0, blocks))\n",
    "        corpus_blocks.append(blocks)\n",
    "        # For each distinct term in the document\n",
    "        for term in set(doc.split(\" \")):\n",
    "            if doc_idx not in idx_list[term].keys():\n",
    "                idx_list[term][doc_idx] = []\n",
    "            # Find occurrences and add block to block list:\n",
    "            for block_idx, block in enumerate(blocks):\n",
    "                if term in block:\n",
    "                    idx_list[term][doc_idx].append(block_idx)\n",
    "\n",
    "    return idx_list, corpus_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'itself': {1: [2]},\n",
       " 'more': {1: [2]},\n",
       " 'but': {1: [1]},\n",
       " 'does': {1: [3]},\n",
       " 'brain': {1: [3]},\n",
       " 'intelligent': {1: [0]},\n",
       " 'people': {1: [0]},\n",
       " 'human': {1: [3]},\n",
       " 'behavior': {1: [0]},\n",
       " 'product': {1: [1]},\n",
       " 'mind': {1: [1, 2]}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_idx_block([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intelligent behavior people product mind but mind itself more human brain does\n",
      "0 ['intelligent', 'behavior', 'people']\n",
      "1 ['product', 'mind', 'but']\n",
      "2 ['mind', 'itself', 'more']\n",
      "3 ['human', 'brain', 'does']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'itself': {1: [2]},\n",
       " 'more': {1: [2]},\n",
       " 'but': {1: [1]},\n",
       " 'does': {1: [3]},\n",
       " 'brain': {1: [3]},\n",
       " 'intelligent': {1: [0]},\n",
       " 'people': {1: [0]},\n",
       " 'human': {1: [3]},\n",
       " 'behavior': {1: [0]},\n",
       " 'product': {1: [1]},\n",
       " 'mind': {1: [1, 2]}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'word': {doc_id: [block_indices]}\n",
    "# Indexing using block addressing\n",
    "\n",
    "print(text)\n",
    "idx, blocks = gen_idx_block([text], block_size=3)\n",
    "for bidx, block in enumerate(blocks[0]):\n",
    "    print(bidx,block)\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Suffix Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary suffix trie\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, data=None, index=-1):\n",
    "        self.children = {}\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "txt = \"missing mississippi\"\n",
    "suffixes = [txt[i:] for i in range(len(txt))]\n",
    "suffixes\n",
    "\n",
    "class SuffixTrie:\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        print(\"test\")\n",
    "\n",
    "    def create_tree(self, txt):\n",
    "        self.root = self.build_tree(txt)\n",
    "        self.root = self.reduce(self.root, txt)\n",
    "\n",
    "    def build_tree(self, txt):\n",
    "        txt += \"$\"\n",
    "        root = Node()\n",
    "        current = root\n",
    "        for i in range(len(txt)):\n",
    "            current = root\n",
    "            for j in range(i, len(txt)):\n",
    "                c = txt[j]\n",
    "                if c not in current.children:\n",
    "                    newNode = Node(index=j-(j-i)+1, data=c if c != \" \" else \"_\")\n",
    "                    current.children[c] = newNode\n",
    "                current = current.children[c]\n",
    "        return root\n",
    "\n",
    "    def search(self, txt, count=0):\n",
    "        current = root\n",
    "        if len(txt) <= 0:\n",
    "            if txt not in current.children:\n",
    "                return False\n",
    "            return current.children[txt].index\n",
    "        for c in txt:\n",
    "            if c in current.children:\n",
    "                current = current.children[c]\n",
    "            else:\n",
    "                return self.search(txt[current.index:len(txt)+1], count+1)\n",
    "        return current.index-1\n",
    "\n",
    "    def reduce(self, current: Node, txt):\n",
    "        idx = self.search()\n",
    "\n",
    "        for child in current.children:\n",
    "            self.reduce(child)\n",
    "\n",
    "root = SuffixTrie()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"missing mississippi\"\n",
    "\n",
    "def search(key, count=0):\n",
    "    print(key)\n",
    "    current = root\n",
    "    if len(key) <= 0:\n",
    "        if key not in current.children:\n",
    "            return False\n",
    "        print(current.children)\n",
    "        return current.children[key].index\n",
    "    for c in key:\n",
    "        if c in current.children:\n",
    "            current = current.children[c]\n",
    "        else:\n",
    "            return search(txt[current.index:len(key)+1], count+1)\n",
    "    return current.index-1\n",
    "\n",
    "# search(\"issing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing on corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['although know much more human brain even',\n",
       " 'ten years ago thinking engages remains pretty much total',\n",
       " 'mystery it big jigsaw puzzle see many',\n",
       " 'pieces put together there much',\n",
       " 'understand all']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Although we know much more about the human brain than we did even\",\n",
    "    \"ten years ago, the thinking it engages in remains pretty much a total\",\n",
    "    \"mystery. It is like a big jigsaw puzzle where we can see many of the\",\n",
    "    \"pieces, but cannot yet put them together. There is so much about us\",\n",
    "    \"that we do not understand at all.\",\n",
    "]\n",
    "corpus = [clean(text) for text in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'more': {1: 1},\n",
       " 'pieces': {4: 1},\n",
       " 'although': {1: 1},\n",
       " 'know': {1: 1},\n",
       " 'many': {3: 1},\n",
       " 'mystery': {3: 1},\n",
       " 'it': {3: 1},\n",
       " 'see': {3: 1},\n",
       " 'together': {4: 1},\n",
       " 'there': {4: 1},\n",
       " 'ten': {2: 1},\n",
       " 'much': {1: 1, 2: 1, 4: 1},\n",
       " 'total': {2: 1},\n",
       " 'remains': {2: 1},\n",
       " 'big': {3: 1},\n",
       " 'brain': {1: 1},\n",
       " 'puzzle': {3: 1},\n",
       " 'thinking': {2: 1},\n",
       " 'pretty': {2: 1},\n",
       " 'all': {5: 1},\n",
       " 'jigsaw': {3: 1},\n",
       " 'understand': {5: 1},\n",
       " 'put': {4: 1},\n",
       " 'engages': {2: 1},\n",
       " 'even': {1: 1},\n",
       " 'years': {2: 1},\n",
       " 'human': {1: 1},\n",
       " 'ago': {2: 1}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index, blocks = gen_idx_block(corpus, block_size=3)\n",
    "index = gen_idx(corpus)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 4: 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index1 = index['much']\n",
    "index1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73696559, 0.73696559, 0.73696559])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = np.array(list(index1.values()))\n",
    "df = len(index1)\n",
    "idf = np.log2(len(corpus) / df)\n",
    "tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "# This is used for preprocessing of both the corpus and queries\n",
    "def preprocessing(text):\n",
    "    # Initiate stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Define unwanted characters (punctuation)\n",
    "    bad_chars = string.punctuation+\"\\n\\r\\t\"\n",
    "\n",
    "    # Clean, tokenize and stem text\n",
    "    new_text = text = text.lower() # all lower case\n",
    "    new_text = \"\".join(list(filter(lambda x: x not in bad_chars, new_text))) # remove unwanted chars\n",
    "    new_text = new_text.split(\" \") # tokenize (split into words)\n",
    "    new_text = list(filter(lambda c: len(c) > 0, new_text)) # remove empty strings\n",
    "    new_text = [stemmer.stem(word) for word in new_text] # perform stemming\n",
    "    new_text = \" \".join(new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whenev': {2: 1},\n",
       " 'heavi': {2: 1},\n",
       " 'wonder': {1: 4},\n",
       " 'nois': {2: 1},\n",
       " 'belli': {2: 2},\n",
       " 'sleep': {2: 4},\n",
       " 'can': {2: 2, 6: 3},\n",
       " 'samsa': {2: 2},\n",
       " 'ital': {4: 3},\n",
       " 'dwell': {1: 3},\n",
       " 'coalesc': {5: 7},\n",
       " 'gild': {2: 1},\n",
       " 'piti': {2: 1},\n",
       " 'convinc': {4: 3},\n",
       " 'translat': {5: 7},\n",
       " 'drew': {2: 1},\n",
       " 'music': {5: 7},\n",
       " 'sink': {1: 3},\n",
       " 'gone': {2: 1},\n",
       " 'deepli': {2: 1},\n",
       " 'funni': {2: 1},\n",
       " 'chest': {2: 1},\n",
       " 'greater': {1: 3, 6: 2},\n",
       " 'wall': {2: 1},\n",
       " 'close': {1: 3, 2: 1},\n",
       " 'fli': {1: 3, 4: 3},\n",
       " 'ha': {1: 4, 4: 3, 6: 6},\n",
       " 'quietli': {2: 2, 3: 3},\n",
       " 'set': {2: 1},\n",
       " 'tell': {2: 1},\n",
       " 'pane': {2: 1},\n",
       " 'how': {2: 1, 3: 6, 6: 6},\n",
       " 'viewer': {2: 1},\n",
       " 'eat': {2: 1},\n",
       " 'littl': {1: 3, 2: 4, 4: 9},\n",
       " 'hi': {1: 3, 2: 20},\n",
       " 'impress': {1: 3},\n",
       " 'let': {2: 1},\n",
       " 'river': {4: 4},\n",
       " 'from': {2: 4, 3: 6, 4: 10, 6: 6},\n",
       " 'wikigirl': {3: 3},\n",
       " 'power': {1: 3, 6: 3},\n",
       " 'fifteen': {2: 1},\n",
       " 'best': {2: 1, 6: 3},\n",
       " 'she': {3: 3, 4: 18},\n",
       " 'they': {4: 10, 6: 3},\n",
       " 'were': {2: 3, 3: 3, 4: 3},\n",
       " 'full': {1: 3},\n",
       " 'put': {2: 1, 4: 3},\n",
       " 'hour': {6: 3},\n",
       " 'concept': {1: 3},\n",
       " 'etern': {1: 3},\n",
       " 'painsbut': {6: 1},\n",
       " 'as': {1: 9, 2: 7, 5: 21, 6: 3},\n",
       " 'never': {1: 3, 2: 3},\n",
       " 'pay': {2: 1, 5: 7},\n",
       " 'vermin': {2: 1},\n",
       " 'didnt': {2: 2},\n",
       " 'skeptic': {5: 7},\n",
       " 'alon': {1: 4},\n",
       " 'belt': {4: 3},\n",
       " 'if': {2: 6, 4: 3, 5: 7},\n",
       " 'heart': {1: 4},\n",
       " 'stream': {1: 3},\n",
       " 'be': {1: 6, 2: 5, 4: 3, 5: 35, 6: 9},\n",
       " 'fop': {3: 3},\n",
       " 'hope': {2: 1},\n",
       " 'woven': {3: 9},\n",
       " 'vex': {3: 21},\n",
       " 'annoy': {6: 6},\n",
       " 'certain': {6: 3},\n",
       " 'spring': {1: 4},\n",
       " 'quickli': {2: 1, 3: 12},\n",
       " 'toil': {6: 6},\n",
       " 'buzz': {1: 3},\n",
       " 'at': {1: 3, 2: 9, 4: 4},\n",
       " 'breath': {1: 3},\n",
       " 'fail': {6: 3},\n",
       " 'heaven': {1: 3, 2: 1},\n",
       " 'o': {1: 3},\n",
       " 'right': {2: 4, 4: 4, 6: 3},\n",
       " 'mirror': {1: 6},\n",
       " 'undertak': {6: 3},\n",
       " 'fox': {3: 22},\n",
       " 'dear': {1: 3},\n",
       " 'give': {6: 3},\n",
       " 'weak': {6: 3},\n",
       " 'say': {6: 3},\n",
       " 'zebra': {3: 6},\n",
       " 'upper': {1: 3},\n",
       " 'member': {5: 7},\n",
       " 'i': {1: 44, 2: 8, 6: 6},\n",
       " 'jodhpur': {3: 3},\n",
       " 'initi': {4: 3},\n",
       " 'report': {2: 2},\n",
       " 'contact': {2: 1},\n",
       " 'had': {2: 6, 4: 3},\n",
       " 'teem': {1: 3},\n",
       " 'lie': {1: 3},\n",
       " 'rais': {2: 1},\n",
       " 'sever': {5: 7},\n",
       " 'untrammel': {6: 3},\n",
       " 'alphabet': {4: 3},\n",
       " 'some': {2: 1, 6: 6},\n",
       " 'suppli': {4: 4},\n",
       " 'quack': {3: 12},\n",
       " 'met': {4: 3},\n",
       " 'size': {2: 1},\n",
       " 'jug': {3: 3},\n",
       " 'understand': {2: 1},\n",
       " 'toward': {2: 2},\n",
       " 'gleam': {1: 3},\n",
       " 'confound': {3: 3},\n",
       " 'countless': {1: 3},\n",
       " 'readi': {2: 1},\n",
       " 'copi': {2: 1, 4: 12},\n",
       " 'made': {2: 1, 4: 6},\n",
       " 'anger': {2: 1},\n",
       " 'hundr': {2: 1},\n",
       " 'chump': {3: 3},\n",
       " 'assist': {2: 2},\n",
       " 'behind': {4: 4},\n",
       " 'pack': {2: 1, 4: 3},\n",
       " 'slept': {2: 1},\n",
       " 'bliss': {1: 7},\n",
       " 'lazi': {2: 1, 3: 7},\n",
       " 'out': {2: 7},\n",
       " 'indescrib': {1: 3},\n",
       " 'bright': {3: 3},\n",
       " 'guest': {2: 1},\n",
       " 'sit': {2: 2},\n",
       " 'recommend': {2: 1},\n",
       " 'man': {2: 1, 6: 6},\n",
       " 'regelialia': {4: 4},\n",
       " 'he': {2: 40, 6: 4},\n",
       " 'indign': {6: 3},\n",
       " 'mad': {2: 1},\n",
       " 'teach': {6: 3},\n",
       " 'cambridg': {5: 7},\n",
       " 'posit': {2: 2},\n",
       " 'letter': {3: 3},\n",
       " 'possibl': {2: 1},\n",
       " 'float': {1: 3},\n",
       " 'jumbl': {3: 3},\n",
       " 'headlin': {4: 3},\n",
       " 'wolv': {3: 3},\n",
       " 'cold': {2: 1},\n",
       " 'compani': {2: 1},\n",
       " 'grab': {3: 3},\n",
       " 'is': {1: 9, 2: 2, 3: 3, 4: 6, 5: 19, 6: 18},\n",
       " 'ought': {2: 1},\n",
       " 'show': {2: 1, 3: 3},\n",
       " 'flock': {3: 6},\n",
       " 'whang': {3: 3},\n",
       " 'id': {2: 3},\n",
       " 'shudder': {2: 1},\n",
       " 'etc': {5: 7},\n",
       " 'warm': {1: 3},\n",
       " 'roll': {2: 1},\n",
       " 'differ': {2: 1, 5: 7},\n",
       " 'unorthograph': {4: 3},\n",
       " 'sixti': {3: 3},\n",
       " 'myth': {5: 7},\n",
       " 'view': {4: 3},\n",
       " 'actual': {6: 3},\n",
       " 'mock': {3: 3},\n",
       " 'bound': {6: 3},\n",
       " 'must': {2: 2, 6: 3},\n",
       " 'hard': {2: 2},\n",
       " 'upon': {1: 3},\n",
       " 'so': {1: 9, 2: 3, 4: 6, 6: 6},\n",
       " 'still': {2: 3, 4: 3},\n",
       " 'listen': {4: 3},\n",
       " 'singl': {1: 3},\n",
       " 'human': {2: 1, 6: 3},\n",
       " 'vocabulari': {5: 7},\n",
       " 'procur': {6: 3},\n",
       " 'adjust': {3: 3},\n",
       " 'quip': {3: 3},\n",
       " 'flounder': {2: 1},\n",
       " 'warn': {4: 3},\n",
       " 'control': {4: 3},\n",
       " 'especi': {2: 1},\n",
       " 'these': {1: 10, 2: 1, 6: 6},\n",
       " 'masterbuild': {6: 3},\n",
       " 'part': {4: 3},\n",
       " 'stray': {1: 3},\n",
       " 'blowzi': {3: 3},\n",
       " 'dim': {3: 3},\n",
       " 'regular': {5: 14},\n",
       " 'to': {1: 6, 2: 30, 3: 9, 4: 9, 5: 28, 6: 49},\n",
       " 'hell': {2: 1},\n",
       " 'waltz': {3: 9},\n",
       " 'wild': {4: 3},\n",
       " 'believ': {2: 1},\n",
       " 'roast': {4: 3},\n",
       " 'girl': {3: 3},\n",
       " 'pronunci': {5: 14},\n",
       " 'entir': {1: 4, 2: 1},\n",
       " 'blew': {3: 3},\n",
       " 'secur': {6: 2},\n",
       " 'waxth': {3: 1},\n",
       " 'duden': {4: 4},\n",
       " 'quit': {2: 1, 3: 3},\n",
       " 'turn': {2: 1, 4: 3},\n",
       " 'welcom': {6: 3},\n",
       " 'furniturerattl': {2: 1},\n",
       " 'gentlemen': {2: 1},\n",
       " 'insect': {1: 3},\n",
       " 'proper': {2: 1},\n",
       " 'rain': {2: 1},\n",
       " 'by': {1: 6, 2: 2, 3: 15, 4: 7, 6: 6},\n",
       " 'travel': {2: 4},\n",
       " 'away': {4: 4},\n",
       " 'skylin': {4: 3},\n",
       " 'hometown': {4: 3},\n",
       " 'demor': {6: 3},\n",
       " 'coast': {4: 4},\n",
       " 'came': {4: 3},\n",
       " 'which': {1: 11, 2: 2, 4: 3, 6: 9},\n",
       " 'mistaken': {6: 3},\n",
       " 'fun': {3: 3},\n",
       " 'uniform': {5: 7},\n",
       " 'time': {2: 5, 4: 3},\n",
       " 'workshi': {2: 1},\n",
       " 'oclock': {2: 2},\n",
       " 'europ': {5: 7},\n",
       " 'baz': {3: 3},\n",
       " 'rewritten': {4: 6},\n",
       " 'ive': {2: 3},\n",
       " 'herfar': {4: 1},\n",
       " 'zephyr': {3: 9},\n",
       " 'zap': {3: 3},\n",
       " 'been': {2: 5, 4: 6},\n",
       " 'woke': {2: 1},\n",
       " 'home': {2: 1},\n",
       " 'bear': {1: 3},\n",
       " 'zani': {3: 3},\n",
       " 'one': {2: 2, 4: 3, 5: 7, 6: 6},\n",
       " 'half': {2: 2},\n",
       " 'frame': {2: 1},\n",
       " 'mountain': {4: 7},\n",
       " 'pity': {4: 3},\n",
       " 'galvan': {3: 3},\n",
       " 'consonantia': {4: 4},\n",
       " 'separ': {4: 4, 5: 7},\n",
       " 'ghost': {3: 3},\n",
       " 'tranquil': {1: 3},\n",
       " 'owe': {6: 3},\n",
       " 'wise': {6: 3},\n",
       " 'hardli': {2: 1},\n",
       " 'nor': {6: 3},\n",
       " 'hold': {6: 3},\n",
       " 'troubl': {2: 1, 6: 3},\n",
       " 'onli': {2: 1, 5: 7},\n",
       " 'game': {3: 3},\n",
       " 'zipper': {3: 3},\n",
       " 'becom': {2: 1},\n",
       " 'talk': {2: 1},\n",
       " 'threw': {2: 1},\n",
       " 'presenc': {1: 3},\n",
       " 'jeopardi': {3: 3},\n",
       " 'charm': {1: 4, 6: 3},\n",
       " 'bow': {3: 3},\n",
       " 'pursu': {6: 6},\n",
       " 'everyth': {2: 1, 4: 3},\n",
       " 'jinx': {3: 3},\n",
       " 'project': {4: 3},\n",
       " 'new': {5: 14},\n",
       " 'avoid': {2: 1, 6: 11},\n",
       " 'earthquak': {3: 3},\n",
       " 'alex': {3: 3},\n",
       " 'choos': {6: 3},\n",
       " 'jumpi': {3: 3},\n",
       " 'hear': {1: 3, 2: 1},\n",
       " 'villag': {4: 3},\n",
       " 'enjoy': {1: 4, 6: 3},\n",
       " 'touch': {2: 1},\n",
       " 'present': {1: 3, 2: 1},\n",
       " 'hand': {2: 1, 6: 3},\n",
       " 'reject': {6: 5},\n",
       " 'brick': {3: 3},\n",
       " 'stalk': {1: 3},\n",
       " 'flick': {3: 3},\n",
       " 'ocean': {4: 4},\n",
       " 'seem': {1: 3, 2: 1, 5: 7},\n",
       " 'decid': {4: 3},\n",
       " 'heard': {2: 1},\n",
       " 'belov': {1: 3},\n",
       " 'possess': {1: 4},\n",
       " 'desir': {5: 7, 6: 6},\n",
       " 'gunboat': {3: 3},\n",
       " 'sublin': {4: 3},\n",
       " 'live': {1: 3, 2: 2, 4: 8},\n",
       " 'waft': {3: 3},\n",
       " 'fowl': {3: 6},\n",
       " 'iraq': {3: 3},\n",
       " 'recent': {2: 1},\n",
       " 'alway': {2: 2, 6: 3},\n",
       " 'veldt': {3: 3},\n",
       " 'servic': {2: 1},\n",
       " 'mine': {1: 3, 5: 7},\n",
       " 'simpl': {5: 21, 6: 3},\n",
       " 'morn': {1: 4, 2: 2},\n",
       " 'throw': {1: 3},\n",
       " 'headboard': {2: 1},\n",
       " 'armourlik': {2: 1},\n",
       " 'sanctuari': {1: 3},\n",
       " 'room': {2: 2},\n",
       " 'instanc': {2: 1},\n",
       " 'around': {1: 6, 4: 3},\n",
       " 'magazin': {2: 1},\n",
       " 'particularli': {2: 1},\n",
       " 'get': {2: 6, 3: 3},\n",
       " 'irregular': {2: 1},\n",
       " 'spot': {1: 4, 2: 2},\n",
       " 'happi': {1: 3, 6: 3},\n",
       " 'befor': {2: 1, 3: 3},\n",
       " 'quiz': {3: 33},\n",
       " 'long': {1: 3, 2: 2, 4: 6},\n",
       " 'driven': {3: 3},\n",
       " 'watch': {3: 6},\n",
       " 'would': {1: 3, 2: 9, 4: 6, 5: 14},\n",
       " 'them': {2: 1},\n",
       " 'better': {2: 1},\n",
       " 'with': {1: 13, 2: 9, 4: 7, 6: 6},\n",
       " 'into': {1: 3, 2: 4, 4: 9},\n",
       " 'those': {6: 6},\n",
       " 'food': {2: 1},\n",
       " 'divid': {2: 1},\n",
       " 'far': {4: 14},\n",
       " 'dislik': {6: 6},\n",
       " 'boa': {2: 1},\n",
       " 'fredericka': {3: 3},\n",
       " 'between': {2: 1},\n",
       " 'bawd': {3: 3},\n",
       " 'moment': {1: 3, 2: 1, 6: 3},\n",
       " 'spread': {2: 1},\n",
       " 'dull': {2: 2},\n",
       " 'red': {3: 3},\n",
       " 'prog': {3: 3},\n",
       " 'semant': {4: 4},\n",
       " 'overspread': {1: 3},\n",
       " 'almighti': {1: 3},\n",
       " 'weight': {1: 3},\n",
       " 'choic': {6: 3},\n",
       " 'plaid': {3: 3},\n",
       " 'oblig': {6: 3},\n",
       " 'forc': {3: 3},\n",
       " 'famili': {5: 7},\n",
       " 'arm': {2: 1},\n",
       " 'sort': {2: 1},\n",
       " 'money': {2: 1},\n",
       " 'who': {1: 3, 2: 2, 6: 21},\n",
       " 'suppos': {2: 1},\n",
       " 'kvetch': {3: 3},\n",
       " 'think': {1: 3, 2: 2},\n",
       " 'prevent': {6: 3},\n",
       " 'dark': {1: 3},\n",
       " 'occident': {5: 21},\n",
       " 'thing': {2: 1},\n",
       " 'herself': {4: 3},\n",
       " 'occasion': {6: 3},\n",
       " 'when': {1: 9, 2: 5, 3: 3, 4: 3, 6: 6},\n",
       " 'cover': {2: 3},\n",
       " 'pick': {3: 6},\n",
       " 'joaquin': {3: 3},\n",
       " 'you': {2: 3, 6: 6},\n",
       " 'advantag': {6: 3},\n",
       " 'muff': {2: 1},\n",
       " 'forese': {6: 3},\n",
       " 'few': {1: 3, 3: 6, 4: 3},\n",
       " 'grow': {1: 3},\n",
       " 'meridian': {1: 3},\n",
       " 'collect': {2: 2},\n",
       " 'are': {1: 3, 2: 1, 4: 3, 5: 7, 6: 12},\n",
       " 'wove': {3: 3},\n",
       " 'veri': {3: 6},\n",
       " 'leav': {2: 1, 4: 3},\n",
       " 'fog': {3: 3},\n",
       " 'european': {5: 14},\n",
       " 'gregor': {2: 5},\n",
       " 'catch': {2: 2},\n",
       " 'compar': {2: 1},\n",
       " 'white': {2: 1},\n",
       " 'vow': {3: 3},\n",
       " 'within': {1: 3},\n",
       " 'exchang': {3: 3},\n",
       " 'back': {2: 6, 4: 3},\n",
       " 'abus': {4: 3},\n",
       " 'year': {2: 2},\n",
       " 'lot': {2: 1},\n",
       " 'result': {5: 7, 6: 3},\n",
       " 'thin': {2: 1},\n",
       " 'ambush': {4: 3},\n",
       " 'belong': {6: 3},\n",
       " 'past': {2: 2},\n",
       " 'jay': {3: 3},\n",
       " 'juli': {3: 3},\n",
       " 'dome': {2: 1},\n",
       " 'tv': {3: 9},\n",
       " 'mtv': {3: 9},\n",
       " 'alarm': {2: 2},\n",
       " 'flow': {4: 4},\n",
       " 'simplifi': {5: 7},\n",
       " 'hasnâ€™t': {4: 3},\n",
       " 'most': {5: 7},\n",
       " 'mere': {1: 3},\n",
       " 'becaus': {2: 4, 4: 3, 6: 12},\n",
       " 'parol': {4: 3},\n",
       " 'forget': {2: 1},\n",
       " 'though': {2: 1},\n",
       " 'no': {2: 1, 3: 3, 4: 3, 6: 9},\n",
       " 'judg': {3: 3},\n",
       " 'shrink': {6: 3},\n",
       " 'felt': {2: 2},\n",
       " 'lane': {4: 3},\n",
       " 'effort': {2: 1},\n",
       " 'own': {1: 3, 2: 1, 4: 6},\n",
       " 'ax': {3: 3},\n",
       " 'bookmarksgrov': {4: 7},\n",
       " 'individu': {5: 7},\n",
       " 'familiar': {1: 3, 2: 1},\n",
       " 'creat': {1: 4},\n",
       " 'sat': {2: 1},\n",
       " 'have': {2: 13, 3: 3, 4: 3, 5: 7, 6: 3},\n",
       " 'salesman': {2: 1},\n",
       " 'return': {4: 3},\n",
       " 'same': {5: 14, 6: 3},\n",
       " 'subordin': {2: 1},\n",
       " 'men': {6: 3},\n",
       " 'feel': {1: 10, 2: 5},\n",
       " 'jock': {3: 3},\n",
       " 'ladi': {2: 1},\n",
       " 'cozi': {3: 3},\n",
       " 'beguil': {6: 3},\n",
       " 'tabl': {2: 1},\n",
       " 'seiz': {3: 3},\n",
       " 'larg': {4: 4},\n",
       " 'more': {2: 4, 5: 21},\n",
       " 'movement': {3: 3},\n",
       " 'but': {1: 6, 2: 6, 4: 6, 6: 14},\n",
       " 'sport': {5: 7},\n",
       " 'world': {1: 3, 4: 3},\n",
       " 'suspici': {2: 1},\n",
       " 'connect': {2: 1},\n",
       " 'wors': {6: 2},\n",
       " 'claim': {2: 1, 6: 3},\n",
       " 'anyon': {2: 1, 6: 3},\n",
       " 'talent': {1: 3},\n",
       " 'languag': {4: 4, 5: 56},\n",
       " 'origin': {4: 3},\n",
       " 'us': {1: 9, 6: 3},\n",
       " 'strenuou': {2: 1},\n",
       " 'dozen': {3: 3},\n",
       " 'labori': {6: 3},\n",
       " 'see': {2: 3},\n",
       " 'impenetr': {1: 3},\n",
       " 'grass': {1: 3},\n",
       " 'her': {2: 1, 4: 41},\n",
       " 'distinguish': {6: 3},\n",
       " 'our': {6: 6},\n",
       " 'lower': {2: 1},\n",
       " 'twenti': {3: 3},\n",
       " 'slide': {2: 1},\n",
       " 'itch': {2: 2},\n",
       " 'way': {4: 9},\n",
       " 'onc': {2: 2},\n",
       " 'not': {2: 7, 4: 3, 6: 3},\n",
       " 'ani': {2: 2, 6: 3},\n",
       " 'youv': {2: 1},\n",
       " 'discothequ': {3: 3},\n",
       " 'bad': {2: 1, 3: 9, 4: 3},\n",
       " 'come': {2: 1},\n",
       " 'inner': {1: 3},\n",
       " 'than': {1: 3, 2: 2, 5: 14},\n",
       " 'strain': {2: 1},\n",
       " 'hat': {2: 1},\n",
       " 'spineless': {2: 1},\n",
       " 'over': {2: 1, 3: 7, 4: 3},\n",
       " 'rush': {2: 1},\n",
       " 'noon': {2: 1},\n",
       " 'scienc': {5: 7},\n",
       " 'your': {2: 3, 4: 3},\n",
       " 'daft': {3: 6},\n",
       " 'pled': {3: 3},\n",
       " 'brown': {2: 1, 3: 7},\n",
       " 'isth': {5: 1},\n",
       " 'tri': {2: 3},\n",
       " 'valley': {1: 3},\n",
       " 'jewel': {3: 3},\n",
       " 'about': {2: 6, 4: 3},\n",
       " 'english': {5: 14},\n",
       " 'easi': {6: 3},\n",
       " 'that': {1: 15, 2: 22, 4: 6, 5: 7, 6: 18},\n",
       " 'head': {2: 2},\n",
       " 'last': {4: 3},\n",
       " 'enough': {2: 1},\n",
       " 'bag': {3: 3},\n",
       " 'look': {2: 4},\n",
       " 'or': {2: 2, 6: 17},\n",
       " 'rethor': {4: 3},\n",
       " 'whi': {5: 7},\n",
       " 'salesmen': {2: 1},\n",
       " 'my': {1: 38, 2: 5, 3: 24},\n",
       " 'now': {1: 3, 2: 1, 3: 3},\n",
       " 'insur': {2: 1},\n",
       " 'jolt': {3: 3},\n",
       " 'absorb': {1: 6},\n",
       " 'earli': {2: 1},\n",
       " 'like': {1: 10, 2: 3, 5: 7, 6: 3},\n",
       " 'visionsa': {1: 1},\n",
       " 'curs': {2: 1},\n",
       " 'sentenc': {4: 3},\n",
       " 'doctor': {2: 3},\n",
       " 'univers': {1: 3},\n",
       " 'career': {2: 1},\n",
       " 'born': {6: 3},\n",
       " 'thousand': {1: 3, 4: 6},\n",
       " 'box': {3: 3},\n",
       " 'seren': {1: 4},\n",
       " 'rung': {2: 2},\n",
       " 'jute': {3: 3},\n",
       " 'first': {2: 1, 4: 3},\n",
       " 'life': {2: 1, 4: 3},\n",
       " 'sexcharg': {3: 3},\n",
       " 'won': {3: 3},\n",
       " 'draw': {1: 3},\n",
       " 'chang': {2: 1},\n",
       " 'circumst': {6: 6},\n",
       " 'him': {2: 6, 3: 3, 6: 3},\n",
       " 'fight': {3: 3},\n",
       " 'exampl': {6: 3},\n",
       " 'pain': {2: 1, 6: 30},\n",
       " 'describ': {1: 3},\n",
       " 'hous': {2: 2},\n",
       " 'vision': {1: 2},\n",
       " 'six': {2: 2, 3: 12},\n",
       " 'strength': {1: 3},\n",
       " 'person': {5: 7},\n",
       " 'foxi': {3: 3},\n",
       " 'medic': {2: 1},\n",
       " 'cannot': {6: 3},\n",
       " 'experiment': {3: 3},\n",
       " 'complet': {6: 3},\n",
       " 'produc': {6: 3},\n",
       " 'longer': {2: 1},\n",
       " 'bosss': {2: 2},\n",
       " 'kill': {3: 3},\n",
       " 'often': {1: 3},\n",
       " 'do': {2: 5, 4: 3, 6: 6},\n",
       " 'taken': {1: 4},\n",
       " 'abov': {2: 1},\n",
       " 'bought': {3: 3},\n",
       " 'go': {2: 4},\n",
       " 'obtain': {6: 6},\n",
       " 'ever': {2: 1, 6: 3},\n",
       " 'stroke': {1: 3},\n",
       " 'nymph': {3: 9},\n",
       " 'grace': {3: 3},\n",
       " 'peopl': {2: 1},\n",
       " 'know': {2: 4, 6: 3},\n",
       " 'tick': {2: 1},\n",
       " 'through': {2: 1, 6: 6},\n",
       " 'could': {1: 6, 2: 4, 4: 3, 5: 7},\n",
       " 'small': {2: 1, 4: 7},\n",
       " 'sphinx': {3: 3},\n",
       " 'milk': {3: 3},\n",
       " 'trivial': {6: 3},\n",
       " 'where': {2: 2, 4: 6},\n",
       " 'probabl': {2: 1},\n",
       " 'pictur': {2: 1},\n",
       " 'parent': {2: 3},\n",
       " 'there': {2: 9, 4: 7, 6: 3},\n",
       " 'friendli': {2: 1},\n",
       " 'exercis': {6: 3},\n",
       " 'quiver': {3: 3},\n",
       " 'too': {1: 3, 2: 1},\n",
       " 'until': {4: 3},\n",
       " 'slid': {2: 1},\n",
       " 'cabl': {3: 3},\n",
       " 'find': {6: 3},\n",
       " 'splendour': {1: 3},\n",
       " 'principl': {6: 3},\n",
       " 'on': {2: 8, 4: 9, 6: 3},\n",
       " 'amazingli': {3: 3},\n",
       " 'safe': {4: 3},\n",
       " 'ration': {6: 3},\n",
       " 'dream': {2: 2},\n",
       " 'nonsens': {2: 1},\n",
       " 'abl': {2: 1, 6: 3},\n",
       " 'job': {3: 6},\n",
       " 'vokalia': {4: 4},\n",
       " 'consequ': {6: 6},\n",
       " 'should': {1: 3, 2: 2, 4: 3},\n",
       " 'next': {2: 1},\n",
       " 'joke': {3: 3},\n",
       " 'off': {2: 3},\n",
       " 'fresh': {2: 1},\n",
       " 'sun': {1: 3},\n",
       " 'mild': {2: 1},\n",
       " 'four': {2: 2},\n",
       " 'mistress': {1: 3},\n",
       " 'mani': {2: 2, 3: 3},\n",
       " 'said': {4: 3},\n",
       " 'perfectli': {6: 3},\n",
       " 'and': {1: 43, 2: 26, 3: 15, 4: 44, 5: 28, 6: 42},\n",
       " 'lift': {2: 2},\n",
       " 'thi': {1: 4, 2: 3, 5: 7, 6: 6},\n",
       " 'definit': {2: 1},\n",
       " 'me': {1: 9, 2: 2, 5: 7},\n",
       " 'contract': {2: 1},\n",
       " 'idea': {6: 3},\n",
       " 'opal': {3: 3},\n",
       " 'quarter': {2: 1},\n",
       " 'himself': {2: 3},\n",
       " 'silk': {3: 3},\n",
       " 'itself': {6: 6},\n",
       " 'plant': {1: 3},\n",
       " 'lay': {2: 3},\n",
       " 'agenc': {4: 3},\n",
       " 'offic': {2: 2},\n",
       " 'amaz': {3: 3},\n",
       " 'jig': {3: 3},\n",
       " 'quickjiv': {3: 3},\n",
       " 'fax': {3: 9},\n",
       " 'ball': {3: 3},\n",
       " 'wax': {3: 5},\n",
       " 'quick': {3: 25},\n",
       " 'vapour': {1: 3},\n",
       " 'much': {1: 3, 2: 1},\n",
       " 'cheek': {4: 3},\n",
       " 'then': {1: 9, 2: 1, 4: 6},\n",
       " 'up': {2: 8, 3: 6},\n",
       " 'jacket': {3: 3},\n",
       " 'reach': {4: 3},\n",
       " 'section': {2: 1},\n",
       " 'sick': {2: 1},\n",
       " 'sens': {1: 3},\n",
       " 'state': {2: 1},\n",
       " 'a': {1: 18, 2: 23, 3: 25, 4: 29, 5: 21, 6: 18},\n",
       " 'place': {2: 1, 4: 4},\n",
       " 'brawni': {3: 3},\n",
       " 'big': {2: 1, 3: 12, 4: 3},\n",
       " 'ipsum': {4: 3},\n",
       " 'artist': {1: 3},\n",
       " 'neglect': {1: 3},\n",
       " 'thought': {2: 5},\n",
       " 'blind': {4: 19, 6: 3},\n",
       " 'found': {2: 2},\n",
       " 'given': {2: 1},\n",
       " 'semikoli': {4: 3},\n",
       " 'provid': {3: 3},\n",
       " 'move': {2: 1, 3: 3},\n",
       " 'while': {1: 3},\n",
       " 'fur': {2: 3},\n",
       " 'soon': {2: 1},\n",
       " 'all': {1: 3, 2: 7, 3: 3, 6: 3},\n",
       " 'hung': {2: 1},\n",
       " 'continu': {4: 3},\n",
       " 'later': {2: 1},\n",
       " 'imag': {1: 3},\n",
       " 'whelp': {3: 3},\n",
       " 'earth': {1: 6},\n",
       " 'yet': {1: 3, 2: 1},\n",
       " 'day': {2: 2, 4: 3},\n",
       " 'jack': {3: 6},\n",
       " 'stupid': {2: 1},\n",
       " 'drawer': {2: 1},\n",
       " 'even': {2: 2, 4: 3},\n",
       " 'round': {2: 1},\n",
       " 'physic': {6: 3},\n",
       " 'zompyc1': {3: 3},\n",
       " 'eye': {1: 3, 2: 1},\n",
       " 'dure': {2: 1},\n",
       " 'mayb': {2: 1},\n",
       " 'foliag': {1: 3},\n",
       " 'exquisit': {1: 3, 3: 3},\n",
       " 'push': {2: 1},\n",
       " 'forward': {2: 1},\n",
       " 'noth': {4: 3, 6: 3},\n",
       " 'although': {2: 1},\n",
       " 'nice': {2: 1},\n",
       " 'occur': {6: 6},\n",
       " 'crazi': {3: 3},\n",
       " 'necessari': {4: 4, 5: 7},\n",
       " 'we': {6: 6},\n",
       " 'jim': {3: 6},\n",
       " 'certainli': {2: 2},\n",
       " 'gaze': {3: 3},\n",
       " 'former': {2: 1},\n",
       " 'flax': {3: 3},\n",
       " 'point': {4: 3},\n",
       " 'horribl': {2: 1},\n",
       " 'other': {2: 1, 6: 5},\n",
       " 'cut': {2: 1},\n",
       " 'explain': {6: 3},\n",
       " 'shut': {2: 1},\n",
       " 'slowli': {2: 1},\n",
       " 'notic': {1: 3, 2: 1},\n",
       " 'great': {6: 6},\n",
       " 'clock': {2: 2},\n",
       " 'saw': {2: 1},\n",
       " 'common': {5: 28},\n",
       " 'ask': {3: 3},\n",
       " 'got': {2: 3},\n",
       " 'helplessli': {2: 1},\n",
       " 'kick': {2: 1},\n",
       " 'dj': {3: 4},\n",
       " 'brave': {3: 3},\n",
       " 'achiev': {5: 7},\n",
       " 'fault': {6: 3},\n",
       " 'slightli': {2: 1},\n",
       " 'began': {2: 1},\n",
       " 'ran': {4: 3},\n",
       " 'breakfast': {2: 1},\n",
       " 'everyon': {5: 7},\n",
       " 'luxuri': {2: 1},\n",
       " 'dog': {3: 7},\n",
       " 'mark': {4: 3},\n",
       " 'select': {6: 3},\n",
       " 'prais': {6: 3},\n",
       " 'stop': {2: 1},\n",
       " 'flummox': {3: 3},\n",
       " 'jukebox': {3: 3},\n",
       " 'drop': {2: 1},\n",
       " 'didnâ€™t': {4: 6},\n",
       " 'equal': {6: 3},\n",
       " 'road': {4: 3},\n",
       " 'desk': {2: 2},\n",
       " 'comma': {4: 3},\n",
       " 'hill': {4: 3},\n",
       " 'drunk': {4: 3},\n",
       " 'realiz': {5: 7},\n",
       " 'system': {6: 3},\n",
       " 'parson': {3: 3},\n",
       " 'use': {2: 1, 4: 3, 5: 7},\n",
       " 'overcom': {2: 1},\n",
       " 'unabl': {2: 1},\n",
       " 'oxmox': {4: 3},\n",
       " 'explor': {6: 3},\n",
       " 'junk': {3: 6},\n",
       " 'zippi': {3: 3},\n",
       " 'textil': {2: 1},\n",
       " 'under': {1: 3},\n",
       " 'enemi': {3: 3},\n",
       " 'righteou': {6: 3},\n",
       " 'did': {2: 2},\n",
       " 'duti': {6: 6},\n",
       " 'jeopard': {3: 3},\n",
       " 'repudi': {6: 3},\n",
       " 'matter': {6: 3},\n",
       " 'dozi': {3: 3},\n",
       " 'bold': {3: 3},\n",
       " 'what': {2: 9, 5: 7, 6: 3},\n",
       " 'wasnt': {2: 1},\n",
       " 'fall': {2: 1},\n",
       " 'busi': {2: 3, 6: 3},\n",
       " 'well': {2: 1},\n",
       " 'went': {2: 1},\n",
       " 'might': {1: 3, 3: 3},\n",
       " 'cajol': {3: 3},\n",
       " 'wizardâ€™': {3: 3},\n",
       " 'front': {3: 3},\n",
       " 'accus': {2: 1},\n",
       " 'whole': {1: 4, 2: 1},\n",
       " 'countri': {4: 10},\n",
       " 'denounc': {6: 6},\n",
       " 'down': {1: 3, 2: 1},\n",
       " 'mouth': {4: 3},\n",
       " 'worri': {2: 1},\n",
       " 'peac': {2: 2},\n",
       " 'train': {2: 5},\n",
       " 'upright': {2: 1},\n",
       " 'rest': {2: 1},\n",
       " 'love': {1: 6, 6: 3},\n",
       " 'togeth': {2: 1},\n",
       " 'someth': {2: 1},\n",
       " 'everi': {6: 6},\n",
       " 'make': {2: 5},\n",
       " 'lovabl': {3: 3},\n",
       " 'form': {1: 9},\n",
       " 'trebek': {3: 3},\n",
       " 'line': {4: 6},\n",
       " 'account': {6: 3},\n",
       " 'except': {6: 3},\n",
       " 'myself': {1: 3},\n",
       " 'onto': {2: 1},\n",
       " 'their': {2: 1, 4: 10, 5: 28, 6: 3},\n",
       " 'blue': {3: 3},\n",
       " 'refus': {5: 7},\n",
       " 'juri': {3: 3},\n",
       " 'am': {1: 7},\n",
       " 'incap': {1: 3},\n",
       " 'bit': {2: 1},\n",
       " 'deviou': {4: 3},\n",
       " 'five': {2: 3, 3: 6},\n",
       " 'wave': {2: 1, 3: 3},\n",
       " 'couldnt': {2: 1},\n",
       " 'ensu': {6: 3},\n",
       " 'tree': {1: 3},\n",
       " 'exist': {1: 7, 5: 14},\n",
       " 'sampl': {2: 2},\n",
       " 'happen': {2: 1},\n",
       " 'transform': {2: 1},\n",
       " 'debt': {2: 1},\n",
       " 'anoth': {2: 1},\n",
       " 'jump': {3: 16},\n",
       " 'expert': {3: 3},\n",
       " 'accept': {2: 1, 6: 3},\n",
       " 'left': {4: 3},\n",
       " 'blame': {6: 3},\n",
       " 'boss': {2: 4},\n",
       " 'pager': {3: 3},\n",
       " 'text': {4: 19},\n",
       " 'almost': {4: 3},\n",
       " 'phoenix': {3: 3},\n",
       " 'pleasur': {6: 34},\n",
       " 'help': {3: 3},\n",
       " 'take': {2: 1, 4: 3, 6: 3},\n",
       " 'unknown': {1: 3},\n",
       " 'just': {2: 3, 3: 3},\n",
       " 'expens': {5: 7},\n",
       " 'frequent': {6: 3},\n",
       " 'infinit': {1: 3},\n",
       " 'ye': {2: 1},\n",
       " 'son': {2: 1},\n",
       " 'insidi': {4: 3},\n",
       " 'wa': {1: 7, 2: 15, 3: 3, 4: 3, 6: 3},\n",
       " 'tall': {1: 3},\n",
       " 'seven': {2: 2, 4: 3},\n",
       " 'surfac': {1: 3},\n",
       " 'the': {1: 80, 2: 42, 3: 33, 4: 81, 5: 62, 6: 39},\n",
       " 'advis': {4: 3},\n",
       " 'slight': {2: 1},\n",
       " 'sustain': {1: 3},\n",
       " 'extrem': {2: 1, 6: 3},\n",
       " 'japan': {3: 3},\n",
       " 'sweet': {1: 4},\n",
       " 'told': {2: 1, 5: 7},\n",
       " 'for': {1: 7, 2: 3, 3: 12, 4: 6, 5: 7},\n",
       " 'wrong': {2: 1},\n",
       " 'pig': {3: 3},\n",
       " 'friend': {1: 9, 5: 7},\n",
       " 'steal': {1: 3},\n",
       " 'blow': {3: 3},\n",
       " 'chosen': {2: 1},\n",
       " 'fact': {5: 7},\n",
       " 'strike': {1: 3},\n",
       " 'top': {2: 1},\n",
       " 'wouldnt': {2: 1},\n",
       " 'devil': {3: 3},\n",
       " 'ago': {2: 2},\n",
       " 'among': {1: 6},\n",
       " 'grammar': {4: 3, 5: 21},\n",
       " 'expound': {6: 3},\n",
       " 'truth': {6: 3},\n",
       " 'endur': {6: 2},\n",
       " 'vixen': {3: 6},\n",
       " 'howev': {2: 1, 4: 3},\n",
       " 'pyjama': {3: 3},\n",
       " 'lorem': {4: 3},\n",
       " 'again': {4: 6, 6: 3},\n",
       " 'an': {1: 3, 2: 1, 4: 3, 5: 7},\n",
       " 'case': {2: 1, 6: 3},\n",
       " 'it': {1: 12, 2: 19, 4: 26, 5: 28, 6: 12},\n",
       " 'stiff': {2: 1},\n",
       " 'weather': {2: 1},\n",
       " 'fit': {2: 1},\n",
       " 'paper': {1: 3},\n",
       " 'question': {3: 3, 4: 6},\n",
       " 'hit': {2: 1},\n",
       " 'els': {6: 2},\n",
       " 'word': {4: 7, 5: 14},\n",
       " 'therefor': {6: 3},\n",
       " 'will': {3: 3, 5: 28, 6: 9},\n",
       " 'two': {3: 3},\n",
       " 'bed': {2: 3, 3: 3},\n",
       " 'hed': {2: 1},\n",
       " 'in': {1: 16, 2: 9, 3: 6, 4: 7, 5: 14, 6: 15},\n",
       " 'name': {4: 7},\n",
       " 'allpow': {4: 3},\n",
       " 'leg': {2: 3},\n",
       " 'true': {2: 1},\n",
       " 'paradisemat': {4: 3},\n",
       " 'sad': {2: 1},\n",
       " 'w': {3: 3},\n",
       " 'luck': {3: 3},\n",
       " 'of': {1: 58, 2: 20, 3: 9, 4: 31, 5: 28, 6: 42},\n",
       " 'writer': {4: 3},\n",
       " 'ill': {2: 4},\n",
       " 'drag': {4: 3},\n",
       " 'quartz': {3: 6},\n",
       " 'oh': {1: 3, 2: 1},\n",
       " 'illustr': {2: 1},\n",
       " 'encount': {6: 3},\n",
       " 'god': {1: 3, 2: 2, 3: 3},\n",
       " 'window': {2: 1},\n",
       " 'trickl': {1: 3},\n",
       " 'quart': {3: 3},\n",
       " 'soul': {1: 17},\n",
       " 'arch': {2: 1},\n",
       " 'forgot': {3: 3},\n",
       " 'versalia': {4: 3},\n",
       " 'jog': {3: 3},\n",
       " 'free': {6: 3}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(1,7):\n",
    "    with open(f\"./DataAssignment4/Text{i}.txt\") as f:\n",
    "        corpus.append(f.read())\n",
    "corpus = [preprocessing(doc) for doc in corpus]\n",
    "index = gen_idx(corpus)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wa', [1, 2, 3, 4, 6]),\n",
       " ('for', [1, 2, 3, 4, 5]),\n",
       " ('it', [1, 2, 4, 5, 6]),\n",
       " ('is', [1, 2, 3, 4, 5, 6]),\n",
       " ('to', [1, 2, 3, 4, 5, 6]),\n",
       " ('and', [1, 2, 3, 4, 5, 6]),\n",
       " ('a', [1, 2, 3, 4, 5, 6]),\n",
       " ('the', [1, 2, 3, 4, 5, 6]),\n",
       " ('in', [1, 2, 3, 4, 5, 6]),\n",
       " ('of', [1, 2, 3, 4, 5, 6])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(k, list(v.keys())) for k, v in index.items()], key=lambda x: len(x[1]))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_raw_indexes(terms):\n",
    "    indexes = []\n",
    "    for term in terms:\n",
    "        if term in index:\n",
    "            indexes.append(index[term])\n",
    "        else:\n",
    "            indexes.append({})\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('brown AND (fox bear) -(bubble cat)',\n",
       " ['brown', 'fox'],\n",
       " ['bubbl'],\n",
       " '         ')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"brown AND (fox bear) -(bubble cat)\"\n",
    "and_terms = [q.split(\" \")[0] for q in query.split(\" AND \")]\n",
    "and_terms = [preprocessing(q) for q in and_terms]\n",
    "negation_terms = [q.split(\" \")[0] for q in query.split(\"-\")[1:]]\n",
    "negation_terms = [preprocessing(q) for q in negation_terms]\n",
    "query, and_terms, negation_terms, \"         \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown', 'AND', ['fox', 'bear'], 'NOT', ['bubble', 'cat']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def parse_query(query):\n",
    "    # Tokenize the query\n",
    "    query = query.replace(\" -\", \" NOT \")\n",
    "    tokens = re.findall(r'(?:AND|OR|NOT|\\(|\\)|[a-zA-Z0-9]+)', query)\n",
    "    return parse_expression(tokens)\n",
    "\n",
    "def parse_expression(tokens):\n",
    "    current_expression = []\n",
    "\n",
    "    while tokens:\n",
    "        token = tokens.pop(0)\n",
    "\n",
    "        if token == \"(\":\n",
    "            # Start a new nested expression\n",
    "            nested_expression = parse_expression(tokens)\n",
    "            current_expression.append(nested_expression)\n",
    "\n",
    "        elif token == \")\":\n",
    "            # End the current expression\n",
    "            break\n",
    "        else:\n",
    "            current_expression.append(token)\n",
    "\n",
    "    return current_expression\n",
    "\n",
    "# Example usage:\n",
    "parsed_query = parse_query(query)\n",
    "parsed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_process_query(query):\n",
    "    query = preprocessing(query).split(\" \")\n",
    "    return retrieve_raw_indexes(query)\n",
    "index_list = full_process_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id1 = [{2: 1, 3: 7}]\n",
    "query = [\"brown\", \"fox\"]\n",
    "def get_docs_by_intersection(terms):\n",
    "    # Get the indexes of the AND terms\n",
    "    indexes = retrieve_raw_indexes(terms)\n",
    "    indexes = sorted(indexes, key=len) # sort for efficiency\n",
    "    if not len(indexes):\n",
    "        return []\n",
    "    # Accumulate docs if they are in all other terms\n",
    "    docs = []\n",
    "    for key in indexes[0].keys(): # for all docs in first index\n",
    "        if all([key in idx.keys() for idx in indexes]): # the doc is in all other indexes\n",
    "            docs.append(key)\n",
    "    return docs\n",
    "\n",
    "get_docs_by_intersection([\"brown\", \"fox\", \"bear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_docs_by_union(terms):\n",
    "    # Get the indexes of the NOT terms\n",
    "    indexes = retrieve_raw_indexes(terms)\n",
    "    if not len(indexes):\n",
    "        return []\n",
    "    # Accumulate docs of the NOT terms\n",
    "    docs = set()\n",
    "    for index in indexes:\n",
    "        docs.update(index.keys())\n",
    "    return list(docs)\n",
    "\n",
    "get_docs_by_union([\"brown\", \"fox\", \"bear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(term):\n",
    "    term = preprocessing(term)\n",
    "    if term in index:\n",
    "        return set(index[term].keys())\n",
    "    return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown', 'and', ['fox', 'bear'], '-', ['bubble', 'cat']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_query = parse_query(query)\n",
    "parsed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"brown AND (fox) -(bubble cat)\"\n",
    "query = \"enjoy AND bear\"\n",
    "def search_recursive(query, doc_set:set=set()):\n",
    "    current_context = set()\n",
    "    operator = None\n",
    "\n",
    "    for term in query:\n",
    "        if isinstance(term, list):\n",
    "            # Recursively process sub-expression\n",
    "            sub_result = search_recursive(term, doc_set)\n",
    "            if operator == 'AND':\n",
    "                doc_set.intersection_update(sub_result)\n",
    "            elif operator == 'NOT':\n",
    "                doc_set.difference_update(sub_result)\n",
    "            else: # default OR\n",
    "                doc_set.update(sub_result)\n",
    "        elif term in {'AND', 'OR', 'NOT', '-'}:\n",
    "            # Set the current operator\n",
    "            operator = term\n",
    "        else:\n",
    "            # Process term\n",
    "            term_docs = retrieve_docs(term)  # Replace with your actual retrieval function\n",
    "            if operator == 'AND':\n",
    "                if not current_context:\n",
    "                    current_context = term_docs\n",
    "                else:\n",
    "                    current_context.intersection_update(term_docs)\n",
    "            elif operator == 'OR':\n",
    "                current_context.update(term_docs)\n",
    "            elif operator == 'NOT':\n",
    "                current_context.difference_update(term_docs)\n",
    "            else:\n",
    "                current_context.update(term_docs)\n",
    "    \n",
    "    # After processing all terms and operators in the query, update the document set\n",
    "    doc_set.update(current_context)\n",
    "    \n",
    "    return doc_set\n",
    "\n",
    "\n",
    "def search(query):\n",
    "    parsed_query = parse_query(query)\n",
    "    return list(search_recursive(parsed_query))\n",
    "\n",
    "search(query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
