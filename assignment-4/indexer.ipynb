{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs, string\n",
    "\n",
    "with codecs.open(\"common-english-words.txt\", \"r\", \"utf-8\") as f:\n",
    "    common_words = f.read().split(\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intelligent behavior people product mind but mind itself more human brain does'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and clean the text\n",
    "text = \"Intelligent behavior in people is a product of the mind. But the mind itself is more like what the human brain does.\"\n",
    "def clean(text):\n",
    "    # Remove stop words\n",
    "    text = \" \".join(list(filter(lambda x: x not in common_words, text.split(\" \"))))\n",
    "    # Remove punctutation\n",
    "    text = \"\".join(list(filter(lambda x: x not in string.punctuation+\"\\n\\r\\t\\0\", text)))\n",
    "    # To lower case\n",
    "    text = text.lower()\n",
    "    return text\n",
    "text = clean(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'more': (1, 1),\n",
       " 'brain': (1, 1),\n",
       " 'but': (1, 1),\n",
       " 'intelligent': (1, 1),\n",
       " 'does': (1, 1),\n",
       " 'behavior': (1, 1),\n",
       " 'human': (1, 1),\n",
       " 'mind': (1, 2),\n",
       " 'product': (1, 1),\n",
       " 'itself': (1, 1),\n",
       " 'people': (1, 1)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create inverted index\n",
    "vocab = dict([(key, (1, text.count(key))) for key in set(text.split(\" \"))])\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'more': {0: 1},\n",
       " 'brain': {0: 1},\n",
       " 'but': {0: 1},\n",
       " 'intelligent': {0: 1},\n",
       " 'does': {0: 1},\n",
       " 'behavior': {0: 1},\n",
       " 'human': {0: 1},\n",
       " 'mind': {0: 2},\n",
       " 'product': {0: 1},\n",
       " 'itself': {0: 1},\n",
       " 'people': {0: 1}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a (normal) inverted index\n",
    "# For one document this is just a frequency list\n",
    "def gen_idx(corpus):\n",
    "    # Initiate the index as a dict('term', dict('doc', num_occ))\n",
    "    idx_list = dict([(key, {}) for key in set(\" \".join(corpus).split(\" \"))])\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        # Increment number of occurrences for each occurrence\n",
    "        for term in doc.split(\" \"):\n",
    "            if doc_idx not in idx_list[term].keys():\n",
    "                idx_list[term][doc_idx] = 0\n",
    "            idx_list[term][doc_idx] += 1\n",
    "    return idx_list\n",
    "gen_idx([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_idx_block(corpus, block_size=3):\n",
    "    # Initiate the index as a dict('term', dict('doc', [block_ids]))\n",
    "    idx_list = dict([(key, {}) for key in set(\" \".join(corpus).split(\" \"))])\n",
    "    corpus_blocks = []\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        # Generate blocks\n",
    "        blocks = [doc.split(\" \")[block_size*i:block_size*i+block_size] for i in range(len(doc.split(\" \"))//block_size+1)]\n",
    "        blocks = list(filter(lambda x: len(x)>0, blocks))\n",
    "        corpus_blocks.append(blocks)\n",
    "        # For each distinct term in the document\n",
    "        for term in set(doc.split(\" \")):\n",
    "            if doc_idx not in idx_list[term].keys():\n",
    "                idx_list[term][doc_idx] = []\n",
    "            # Find occurrences and add block to block list:\n",
    "            for block_idx, block in enumerate(blocks):\n",
    "                if term in block:\n",
    "                    idx_list[term][doc_idx].append(block_idx)\n",
    "\n",
    "    return idx_list, corpus_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'more': {0: [2]},\n",
       " 'brain': {0: [3]},\n",
       " 'but': {0: [1]},\n",
       " 'intelligent': {0: [0]},\n",
       " 'does': {0: [3]},\n",
       " 'behavior': {0: [0]},\n",
       " 'human': {0: [3]},\n",
       " 'mind': {0: [1, 2]},\n",
       " 'product': {0: [1]},\n",
       " 'itself': {0: [2]},\n",
       " 'people': {0: [0]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_idx_block([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intelligent behavior people product mind but mind itself more human brain does\n",
      "0 ['intelligent', 'behavior', 'people']\n",
      "1 ['product', 'mind', 'but']\n",
      "2 ['mind', 'itself', 'more']\n",
      "3 ['human', 'brain', 'does']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'more': {0: [2]},\n",
       " 'brain': {0: [3]},\n",
       " 'but': {0: [1]},\n",
       " 'intelligent': {0: [0]},\n",
       " 'does': {0: [3]},\n",
       " 'behavior': {0: [0]},\n",
       " 'human': {0: [3]},\n",
       " 'mind': {0: [1, 2]},\n",
       " 'product': {0: [1]},\n",
       " 'itself': {0: [2]},\n",
       " 'people': {0: [0]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'word': {doc_id: [block_indices]}\n",
    "# Indexing using block addressing\n",
    "\n",
    "print(text)\n",
    "idx, blocks = gen_idx_block([text], block_size=3)\n",
    "for bidx, block in enumerate(blocks[0]):\n",
    "    print(bidx,block)\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Suffix Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary suffix trie\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, data=None, index=-1):\n",
    "        self.children = {}\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "txt = \"missing mississippi\"\n",
    "suffixes = [txt[i:] for i in range(len(txt))]\n",
    "suffixes\n",
    "\n",
    "class SuffixTrie:\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        print(\"test\")\n",
    "\n",
    "    def create_tree(self, txt):\n",
    "        self.root = self.build_tree(txt)\n",
    "        self.root = self.reduce(self.root, txt)\n",
    "\n",
    "    def build_tree(self, txt):\n",
    "        txt += \"$\"\n",
    "        root = Node()\n",
    "        current = root\n",
    "        for i in range(len(txt)):\n",
    "            current = root\n",
    "            for j in range(i, len(txt)):\n",
    "                c = txt[j]\n",
    "                if c not in current.children:\n",
    "                    newNode = Node(index=j-(j-i)+1, data=c if c != \" \" else \"_\")\n",
    "                    current.children[c] = newNode\n",
    "                current = current.children[c]\n",
    "        return root\n",
    "\n",
    "    def search(self, txt, count=0):\n",
    "        current = root\n",
    "        if len(txt) <= 0:\n",
    "            if txt not in current.children:\n",
    "                return False\n",
    "            return current.children[txt].index\n",
    "        for c in txt:\n",
    "            if c in current.children:\n",
    "                current = current.children[c]\n",
    "            else:\n",
    "                return self.search(txt[current.index:len(txt)+1], count+1)\n",
    "        return current.index-1\n",
    "\n",
    "    def reduce(self, current: Node, txt):\n",
    "        idx = self.search()\n",
    "\n",
    "        for child in current.children:\n",
    "            self.reduce(child)\n",
    "\n",
    "root = SuffixTrie()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"missing mississippi\"\n",
    "\n",
    "def search(key, count=0):\n",
    "    print(key)\n",
    "    current = root\n",
    "    if len(key) <= 0:\n",
    "        if key not in current.children:\n",
    "            return False\n",
    "        print(current.children)\n",
    "        return current.children[key].index\n",
    "    for c in key:\n",
    "        if c in current.children:\n",
    "            current = current.children[c]\n",
    "        else:\n",
    "            return search(txt[current.index:len(key)+1], count+1)\n",
    "    return current.index-1\n",
    "\n",
    "# search(\"issing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing on corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['although know much more human brain even',\n",
       " 'ten years ago thinking engages remains pretty much total',\n",
       " 'mystery it big jigsaw puzzle see many',\n",
       " 'pieces put together there much',\n",
       " 'understand all']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Although we know much more about the human brain than we did even\",\n",
    "    \"ten years ago, the thinking it engages in remains pretty much a total\",\n",
    "    \"mystery. It is like a big jigsaw puzzle where we can see many of the\",\n",
    "    \"pieces, but cannot yet put them together. There is so much about us\",\n",
    "    \"that we do not understand at all.\",\n",
    "]\n",
    "corpus = [clean(text) for text in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'understand': {4: 1},\n",
       " 'pieces': {3: 1},\n",
       " 'see': {2: 1},\n",
       " 'engages': {1: 1},\n",
       " 'mystery': {2: 1},\n",
       " 'it': {2: 1},\n",
       " 'jigsaw': {2: 1},\n",
       " 'much': {0: 1, 1: 1, 3: 1},\n",
       " 'there': {3: 1},\n",
       " 'more': {0: 1},\n",
       " 'although': {0: 1},\n",
       " 'years': {1: 1},\n",
       " 'put': {3: 1},\n",
       " 'know': {0: 1},\n",
       " 'together': {3: 1},\n",
       " 'many': {2: 1},\n",
       " 'total': {1: 1},\n",
       " 'brain': {0: 1},\n",
       " 'ten': {1: 1},\n",
       " 'all': {4: 1},\n",
       " 'puzzle': {2: 1},\n",
       " 'ago': {1: 1},\n",
       " 'even': {0: 1},\n",
       " 'big': {2: 1},\n",
       " 'remains': {1: 1},\n",
       " 'human': {0: 1},\n",
       " 'thinking': {1: 1},\n",
       " 'pretty': {1: 1}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index, blocks = gen_idx_block(corpus, block_size=3)\n",
    "index = gen_idx(corpus)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 1, 3: 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index1 = index['much']\n",
    "index1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73696559, 0.73696559, 0.73696559])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = np.array(list(index1.values()))\n",
    "df = len(index1)\n",
    "idf = np.log2(len(corpus) / df)\n",
    "tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "# This is used for preprocessing of both the corpus and queries\n",
    "def preprocessing(text):\n",
    "    # Initiate stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Define unwanted characters (punctuation)\n",
    "    bad_chars = string.punctuation+\"\\n\\r\\t\"\n",
    "\n",
    "    # Clean, tokenize and stem text\n",
    "    new_text = text = text.lower() # all lower case\n",
    "    new_text = \"\".join(list(filter(lambda x: x not in bad_chars, new_text))) # remove unwanted chars\n",
    "    new_text = new_text.split(\" \") # tokenize (split into words)\n",
    "    new_text = list(filter(lambda c: len(c) > 0, new_text)) # remove empty strings\n",
    "    new_text = [stemmer.stem(word) for word in new_text] # perform stemming\n",
    "    new_text = \" \".join(new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'differ': {1: 1, 4: 7},\n",
       " 'sentenc': {3: 3},\n",
       " 'must': {1: 2, 5: 3},\n",
       " 'indescrib': {0: 3},\n",
       " 'sens': {0: 3},\n",
       " 'size': {1: 1},\n",
       " 'irregular': {1: 1},\n",
       " 'itch': {1: 2},\n",
       " 'certainli': {1: 2},\n",
       " 'river': {3: 4},\n",
       " 'last': {3: 3},\n",
       " 'he': {1: 40, 5: 4},\n",
       " 'fall': {1: 1},\n",
       " 'true': {1: 1},\n",
       " 'writer': {3: 3},\n",
       " 'member': {4: 7},\n",
       " 'seiz': {2: 3},\n",
       " 'system': {5: 3},\n",
       " 'food': {1: 1},\n",
       " 'sink': {0: 3},\n",
       " 'effort': {1: 1},\n",
       " 'initi': {3: 3},\n",
       " 'think': {0: 3, 1: 2},\n",
       " 'exquisit': {0: 3, 2: 3},\n",
       " 'possess': {0: 4},\n",
       " 'travel': {1: 4},\n",
       " 'sort': {1: 1},\n",
       " 'float': {0: 3},\n",
       " 'vixen': {2: 6},\n",
       " 'dream': {1: 2},\n",
       " 'can': {1: 2, 5: 3},\n",
       " 'far': {3: 14},\n",
       " 'came': {3: 3},\n",
       " 'a': {0: 18, 1: 23, 2: 25, 3: 29, 4: 21, 5: 18},\n",
       " 'window': {1: 1},\n",
       " 'move': {1: 1, 2: 3},\n",
       " 'slid': {1: 1},\n",
       " 'readi': {1: 1},\n",
       " 'frequent': {5: 3},\n",
       " 'plant': {0: 3},\n",
       " 'quiz': {2: 33},\n",
       " 'howev': {1: 1, 3: 3},\n",
       " 'mistress': {0: 3},\n",
       " 'kvetch': {2: 3},\n",
       " 'amaz': {2: 3},\n",
       " 'almost': {3: 3},\n",
       " 'jock': {2: 3},\n",
       " 'get': {1: 6, 2: 3},\n",
       " 'jay': {2: 3},\n",
       " 'achiev': {4: 7},\n",
       " 'hat': {1: 1},\n",
       " 'soul': {0: 17},\n",
       " 'world': {0: 3, 3: 3},\n",
       " 'son': {1: 1},\n",
       " 'until': {3: 3},\n",
       " 'vokalia': {3: 4},\n",
       " 'ran': {3: 3},\n",
       " 'compani': {1: 1},\n",
       " 'oblig': {5: 3},\n",
       " 'easi': {5: 3},\n",
       " 'ever': {1: 1, 5: 3},\n",
       " 'make': {1: 5},\n",
       " 'am': {0: 7},\n",
       " 'lot': {1: 1},\n",
       " 'mock': {2: 3},\n",
       " 'hometown': {3: 3},\n",
       " 'hold': {5: 3},\n",
       " 'never': {0: 3, 1: 3},\n",
       " 'presenc': {0: 3},\n",
       " 'might': {0: 3, 2: 3},\n",
       " 'strength': {0: 3},\n",
       " 'forget': {1: 1},\n",
       " 'fox': {2: 22},\n",
       " 'hous': {1: 2},\n",
       " 'year': {1: 2},\n",
       " 'cajol': {2: 3},\n",
       " 'id': {1: 3},\n",
       " 'recommend': {1: 1},\n",
       " 'desir': {4: 7, 5: 6},\n",
       " 'circumst': {5: 6},\n",
       " 'debt': {1: 1},\n",
       " 'kill': {2: 3},\n",
       " 'arch': {1: 1},\n",
       " 'translat': {4: 7},\n",
       " 'seem': {0: 3, 1: 1, 4: 7},\n",
       " 'ensu': {5: 3},\n",
       " 'long': {0: 3, 1: 2, 3: 6},\n",
       " 'tabl': {1: 1},\n",
       " 'she': {2: 3, 3: 18},\n",
       " 'contract': {1: 1},\n",
       " 'deviou': {3: 3},\n",
       " 'els': {5: 2},\n",
       " 'o': {0: 3},\n",
       " 'you': {1: 3, 5: 6},\n",
       " 'blew': {2: 3},\n",
       " 'actual': {5: 3},\n",
       " 'juli': {2: 3},\n",
       " 'fowl': {2: 6},\n",
       " 'regelialia': {3: 4},\n",
       " 'dim': {2: 3},\n",
       " 'heart': {0: 4},\n",
       " 'becaus': {1: 4, 3: 3, 5: 12},\n",
       " 'life': {1: 1, 3: 3},\n",
       " 'instanc': {1: 1},\n",
       " 'fop': {2: 3},\n",
       " 'of': {0: 58, 1: 20, 2: 9, 3: 31, 4: 28, 5: 42},\n",
       " 'everyth': {1: 1, 3: 3},\n",
       " 'jinx': {2: 3},\n",
       " 'inner': {0: 3},\n",
       " 'won': {2: 3},\n",
       " 'whenev': {1: 1},\n",
       " 'human': {1: 1, 5: 3},\n",
       " 'lane': {3: 3},\n",
       " 'truth': {5: 3},\n",
       " 'noth': {3: 3, 5: 3},\n",
       " 'bold': {2: 3},\n",
       " 'room': {1: 2},\n",
       " 'ive': {1: 3},\n",
       " 'what': {1: 9, 4: 7, 5: 3},\n",
       " 'said': {3: 3},\n",
       " 'again': {3: 6, 5: 3},\n",
       " 'from': {1: 4, 2: 6, 3: 10, 5: 6},\n",
       " 'roast': {3: 3},\n",
       " 'agenc': {3: 3},\n",
       " 'comma': {3: 3},\n",
       " 'got': {1: 3},\n",
       " 'mine': {0: 3, 4: 7},\n",
       " 'illustr': {1: 1},\n",
       " 'past': {1: 2},\n",
       " 'impenetr': {0: 3},\n",
       " 'find': {5: 3},\n",
       " 'steal': {0: 3},\n",
       " 'quickli': {1: 1, 2: 12},\n",
       " 'fresh': {1: 1},\n",
       " 'talk': {1: 1},\n",
       " 'return': {3: 3},\n",
       " 'strenuou': {1: 1},\n",
       " 'bad': {1: 1, 2: 9, 3: 3},\n",
       " 'live': {0: 3, 1: 2, 3: 8},\n",
       " 'mayb': {1: 1},\n",
       " 'though': {1: 1},\n",
       " 'catch': {1: 2},\n",
       " 'procur': {5: 3},\n",
       " 'back': {1: 6, 3: 3},\n",
       " 'medic': {1: 1},\n",
       " 'quack': {2: 12},\n",
       " 'insidi': {3: 3},\n",
       " 'peac': {1: 2},\n",
       " 'bright': {2: 3},\n",
       " 'belli': {1: 2},\n",
       " 'pictur': {1: 1},\n",
       " 'wolv': {2: 3},\n",
       " 'veri': {2: 6},\n",
       " 'tranquil': {0: 3},\n",
       " 'assist': {1: 2},\n",
       " 'waft': {2: 3},\n",
       " 'twenti': {2: 3},\n",
       " 'boa': {1: 1},\n",
       " 'went': {1: 1},\n",
       " 'music': {4: 7},\n",
       " 'wise': {5: 3},\n",
       " 'ha': {0: 4, 3: 3, 5: 6},\n",
       " 'eye': {0: 3, 1: 1},\n",
       " 'roll': {1: 1},\n",
       " 'jeopardi': {2: 3},\n",
       " 'pager': {2: 3},\n",
       " 'one': {1: 2, 3: 3, 4: 7, 5: 6},\n",
       " 'go': {1: 4},\n",
       " 'semikoli': {3: 3},\n",
       " 'hardli': {1: 1},\n",
       " 'artist': {0: 3},\n",
       " 'visionsa': {0: 1},\n",
       " 'brown': {1: 1, 2: 7},\n",
       " 'sat': {1: 1},\n",
       " 'met': {3: 3},\n",
       " 'show': {1: 1, 2: 3},\n",
       " 'experiment': {2: 3},\n",
       " 'present': {0: 3, 1: 1},\n",
       " 'with': {0: 13, 1: 9, 3: 7, 5: 6},\n",
       " 'amazingli': {2: 3},\n",
       " 'question': {2: 3, 3: 6},\n",
       " 'listen': {3: 3},\n",
       " 'morn': {0: 4, 1: 2},\n",
       " 'onto': {1: 1},\n",
       " 'mouth': {3: 3},\n",
       " 'shrink': {5: 3},\n",
       " 'feel': {0: 10, 1: 5},\n",
       " 'separ': {3: 4, 4: 7},\n",
       " 'grammar': {3: 3, 4: 21},\n",
       " 'certain': {5: 3},\n",
       " 'wikigirl': {2: 3},\n",
       " 'larg': {3: 4},\n",
       " 'so': {0: 9, 1: 3, 3: 6, 5: 6},\n",
       " 'head': {1: 2},\n",
       " 'sit': {1: 2},\n",
       " 'person': {4: 7},\n",
       " 'becom': {1: 1},\n",
       " 'alphabet': {3: 3},\n",
       " 'money': {1: 1},\n",
       " 'ax': {2: 3},\n",
       " 'avoid': {1: 1, 5: 11},\n",
       " 'now': {0: 3, 1: 1, 2: 3},\n",
       " 'breath': {0: 3},\n",
       " 'cabl': {2: 3},\n",
       " 'saw': {1: 1},\n",
       " 'top': {1: 1},\n",
       " 'men': {5: 3},\n",
       " 'europ': {4: 7},\n",
       " 'trivial': {5: 3},\n",
       " 'anoth': {1: 1},\n",
       " 'exampl': {5: 3},\n",
       " 'valley': {0: 3},\n",
       " 'wave': {1: 1, 2: 3},\n",
       " 'into': {0: 3, 1: 4, 3: 9},\n",
       " 'fifteen': {1: 1},\n",
       " 'oh': {0: 3, 1: 1},\n",
       " 'lay': {1: 3},\n",
       " 'beguil': {5: 3},\n",
       " 'felt': {1: 2},\n",
       " 'confound': {2: 3},\n",
       " 'trickl': {0: 3},\n",
       " 'six': {1: 2, 2: 12},\n",
       " 'quietli': {1: 2, 2: 3},\n",
       " 'jim': {2: 6},\n",
       " 'turn': {1: 1, 3: 3},\n",
       " 'forgot': {2: 3},\n",
       " 'joaquin': {2: 3},\n",
       " 'cheek': {3: 3},\n",
       " 'seven': {1: 2, 3: 3},\n",
       " 'wonder': {0: 4},\n",
       " 'will': {2: 3, 4: 28, 5: 9},\n",
       " 'onli': {1: 1, 4: 7},\n",
       " 'help': {2: 3},\n",
       " 'possibl': {1: 1},\n",
       " 'vocabulari': {4: 7},\n",
       " 'power': {0: 3, 5: 3},\n",
       " 'hundr': {1: 1},\n",
       " 'expound': {5: 3},\n",
       " 'understand': {1: 1},\n",
       " 'suspici': {1: 1},\n",
       " 'see': {1: 3},\n",
       " 'by': {0: 6, 1: 2, 2: 15, 3: 7, 5: 6},\n",
       " 'spring': {0: 4},\n",
       " 'necessari': {3: 4, 4: 7},\n",
       " 'devil': {2: 3},\n",
       " 'when': {0: 9, 1: 5, 2: 3, 3: 3, 5: 6},\n",
       " 'it': {0: 12, 1: 19, 3: 26, 4: 28, 5: 12},\n",
       " 'anyon': {1: 1, 5: 3},\n",
       " 'whi': {4: 7},\n",
       " 'explain': {5: 3},\n",
       " 'our': {5: 6},\n",
       " 'simpl': {4: 21, 5: 3},\n",
       " 'youv': {1: 1},\n",
       " 'know': {1: 4, 5: 3},\n",
       " 'refus': {4: 7},\n",
       " 'galvan': {2: 3},\n",
       " 'blowzi': {2: 3},\n",
       " 'contact': {1: 1},\n",
       " 'bliss': {0: 7},\n",
       " 'secur': {5: 2},\n",
       " 'few': {0: 3, 2: 6, 3: 3},\n",
       " 'arm': {1: 1},\n",
       " 'fun': {2: 3},\n",
       " 'home': {1: 1},\n",
       " 'whang': {2: 3},\n",
       " 'quickjiv': {2: 3},\n",
       " 'part': {3: 3},\n",
       " 'clock': {1: 2},\n",
       " 'vow': {2: 3},\n",
       " 'jewel': {2: 3},\n",
       " 'simplifi': {4: 7},\n",
       " 'quit': {1: 1, 2: 3},\n",
       " 'languag': {3: 4, 4: 56},\n",
       " 'choic': {5: 3},\n",
       " 'front': {2: 3},\n",
       " 'safe': {3: 3},\n",
       " 'talent': {0: 3},\n",
       " 'name': {3: 7},\n",
       " 'fight': {2: 3},\n",
       " 'out': {1: 7},\n",
       " 'weather': {1: 1},\n",
       " 'him': {1: 6, 2: 3, 5: 3},\n",
       " 'daft': {2: 6},\n",
       " 'were': {1: 3, 2: 3, 3: 3},\n",
       " 'trebek': {2: 3},\n",
       " 'divid': {1: 1},\n",
       " 'dozi': {2: 3},\n",
       " 'later': {1: 1},\n",
       " 'sick': {1: 1},\n",
       " 'flow': {3: 4},\n",
       " 'w': {2: 3},\n",
       " 'provid': {2: 3},\n",
       " 'blame': {5: 3},\n",
       " 'teem': {0: 3},\n",
       " 'univers': {0: 3},\n",
       " 'vapour': {0: 3},\n",
       " 'onc': {1: 2},\n",
       " 'explor': {5: 3},\n",
       " 'ghost': {2: 3},\n",
       " 'fail': {5: 3},\n",
       " 'although': {1: 1},\n",
       " 'produc': {5: 3},\n",
       " 'curs': {1: 1},\n",
       " 'decid': {3: 3},\n",
       " 'thought': {1: 5},\n",
       " 'consonantia': {3: 4},\n",
       " 'pled': {2: 3},\n",
       " 'magazin': {1: 1},\n",
       " 'had': {1: 6, 3: 3},\n",
       " 'toward': {1: 2},\n",
       " 'state': {1: 1},\n",
       " 'dog': {2: 7},\n",
       " 'driven': {2: 3},\n",
       " 'quick': {2: 25},\n",
       " 'stiff': {1: 1},\n",
       " 'hope': {1: 1},\n",
       " 'not': {1: 7, 3: 3, 5: 3},\n",
       " 'ask': {2: 3},\n",
       " 'noon': {1: 1},\n",
       " 'jute': {2: 3},\n",
       " 'impress': {0: 3},\n",
       " 'undertak': {5: 3},\n",
       " 'oclock': {1: 2},\n",
       " 'deepli': {1: 1},\n",
       " 'four': {1: 2},\n",
       " 'began': {1: 1},\n",
       " 'drawer': {1: 1},\n",
       " 'train': {1: 5},\n",
       " 'didnâ€™t': {3: 6},\n",
       " 'pity': {3: 3},\n",
       " 'friend': {0: 9, 4: 7},\n",
       " 'form': {0: 9},\n",
       " 'the': {0: 80, 1: 42, 2: 33, 3: 81, 4: 62, 5: 39},\n",
       " 'insur': {1: 1},\n",
       " 'scienc': {4: 7},\n",
       " 'brave': {2: 3},\n",
       " 'wax': {2: 5},\n",
       " 'suppli': {3: 4},\n",
       " 'reject': {5: 5},\n",
       " 'select': {5: 3},\n",
       " 'paradisemat': {3: 3},\n",
       " 'forese': {5: 3},\n",
       " 'better': {1: 1},\n",
       " 'often': {0: 3},\n",
       " 'ladi': {1: 1},\n",
       " 'continu': {3: 3},\n",
       " 'next': {1: 1},\n",
       " 'choos': {5: 3},\n",
       " 'under': {0: 3},\n",
       " 'ital': {3: 3},\n",
       " 'wizardâ€™': {2: 3},\n",
       " 'nonsens': {1: 1},\n",
       " 'toil': {5: 6},\n",
       " 'enemi': {2: 3},\n",
       " 'lift': {1: 2},\n",
       " 'fog': {2: 3},\n",
       " 'upon': {0: 3},\n",
       " 'therefor': {5: 3},\n",
       " 'neglect': {0: 3},\n",
       " 'skeptic': {4: 7},\n",
       " 'busi': {1: 3, 5: 3},\n",
       " 'tri': {1: 3},\n",
       " 'horribl': {1: 1},\n",
       " 'coalesc': {4: 7},\n",
       " 'lie': {0: 3},\n",
       " 'luck': {2: 3},\n",
       " 'myth': {4: 7},\n",
       " 'wrong': {1: 1},\n",
       " 'section': {1: 1},\n",
       " 'happen': {1: 1},\n",
       " 'away': {3: 4},\n",
       " 'pronunci': {4: 14},\n",
       " 'wors': {5: 2},\n",
       " 'villag': {3: 3},\n",
       " 'headboard': {1: 1},\n",
       " 'but': {0: 6, 1: 6, 3: 6, 5: 14},\n",
       " 'pyjama': {2: 3},\n",
       " 'sixti': {2: 3},\n",
       " 'prais': {5: 3},\n",
       " 'expert': {2: 3},\n",
       " 'especi': {1: 1},\n",
       " 'stop': {1: 1},\n",
       " 'chest': {1: 1},\n",
       " 'two': {2: 3},\n",
       " 'sweet': {0: 4},\n",
       " 'anger': {1: 1},\n",
       " 'shut': {1: 1},\n",
       " 'five': {1: 3, 2: 6},\n",
       " 'frame': {1: 1},\n",
       " 'rethor': {3: 3},\n",
       " 'slide': {1: 1},\n",
       " 'breakfast': {1: 1},\n",
       " 'zippi': {2: 3},\n",
       " 'mark': {3: 3},\n",
       " 'drew': {1: 1},\n",
       " 'zani': {2: 3},\n",
       " 'have': {1: 13, 2: 3, 3: 3, 4: 7, 5: 3},\n",
       " 'report': {1: 2},\n",
       " 'down': {0: 3, 1: 1},\n",
       " 'meridian': {0: 3},\n",
       " 'denounc': {5: 6},\n",
       " 'copi': {1: 1, 3: 12},\n",
       " 'wove': {2: 3},\n",
       " 'troubl': {1: 1, 5: 3},\n",
       " 'offic': {1: 2},\n",
       " 'expens': {4: 7},\n",
       " 'cut': {1: 1},\n",
       " 'weak': {5: 3},\n",
       " 'throw': {0: 3},\n",
       " 'connect': {1: 1},\n",
       " 'occur': {5: 6},\n",
       " 'insect': {0: 3},\n",
       " 'take': {1: 1, 3: 3, 5: 3},\n",
       " 'gone': {1: 1},\n",
       " 'mirror': {0: 6},\n",
       " 'warm': {0: 3},\n",
       " 'himself': {1: 3},\n",
       " 'rest': {1: 1},\n",
       " 'mtv': {2: 9},\n",
       " 'splendour': {0: 3},\n",
       " 'believ': {1: 1},\n",
       " 'subordin': {1: 1},\n",
       " 'lazi': {1: 1, 2: 7},\n",
       " 'love': {0: 6, 5: 3},\n",
       " 'bawd': {2: 3},\n",
       " 'around': {0: 6, 3: 3},\n",
       " 'jolt': {2: 3},\n",
       " 'to': {0: 6, 1: 30, 2: 9, 3: 9, 4: 28, 5: 49},\n",
       " 'through': {1: 1, 5: 6},\n",
       " 'strain': {1: 1},\n",
       " 'servic': {1: 1},\n",
       " 'famili': {4: 7},\n",
       " 'belong': {5: 3},\n",
       " 'i': {0: 44, 1: 8, 5: 6},\n",
       " 'ball': {2: 3},\n",
       " 'off': {1: 3},\n",
       " 'rain': {1: 1},\n",
       " 'dislik': {5: 6},\n",
       " 'waltz': {2: 9},\n",
       " 'japan': {2: 3},\n",
       " 'cannot': {5: 3},\n",
       " 'cold': {1: 1},\n",
       " 'leav': {1: 1, 3: 3},\n",
       " 'bought': {2: 3},\n",
       " 'quarter': {1: 1},\n",
       " 'parson': {2: 3},\n",
       " 'thousand': {0: 3, 3: 6},\n",
       " 'upper': {0: 3},\n",
       " 'grass': {0: 3},\n",
       " 'slight': {1: 1},\n",
       " 'cover': {1: 3},\n",
       " 'mani': {1: 2, 2: 3},\n",
       " 'right': {1: 4, 3: 4, 5: 3},\n",
       " 'jeopard': {2: 3},\n",
       " 'did': {1: 2},\n",
       " 'as': {0: 9, 1: 7, 4: 21, 5: 3},\n",
       " 'say': {5: 3},\n",
       " 'distinguish': {5: 3},\n",
       " 'fur': {1: 3},\n",
       " 'pane': {1: 1},\n",
       " 'hasnâ€™t': {3: 3},\n",
       " 'these': {0: 10, 1: 1, 5: 6},\n",
       " 'are': {0: 3, 1: 1, 3: 3, 4: 7, 5: 12},\n",
       " 'upright': {1: 1},\n",
       " 'everi': {5: 6},\n",
       " 'bed': {1: 3, 2: 3},\n",
       " 'didnt': {1: 2},\n",
       " 'repudi': {5: 3},\n",
       " 'woven': {2: 9},\n",
       " 'heard': {1: 1},\n",
       " 'countri': {3: 10},\n",
       " 'textil': {1: 1},\n",
       " 'semant': {3: 4},\n",
       " 'masterbuild': {5: 3},\n",
       " 'exchang': {2: 3},\n",
       " 'obtain': {5: 6},\n",
       " 'skylin': {3: 3},\n",
       " 'befor': {1: 1, 2: 3},\n",
       " 'unknown': {0: 3},\n",
       " 'vex': {2: 21},\n",
       " 'made': {1: 1, 3: 6},\n",
       " 'behind': {3: 4},\n",
       " 'fli': {0: 3, 3: 3},\n",
       " 'proper': {1: 1},\n",
       " 'nois': {1: 1},\n",
       " 'movement': {2: 3},\n",
       " 'my': {0: 38, 1: 5, 2: 24},\n",
       " 'peopl': {1: 1},\n",
       " 'on': {1: 8, 3: 9, 5: 3},\n",
       " 'gaze': {2: 3},\n",
       " 'alex': {2: 3},\n",
       " 'mere': {0: 3},\n",
       " 'salesmen': {1: 1},\n",
       " 'bow': {2: 3},\n",
       " 'belt': {3: 3},\n",
       " 'would': {0: 3, 1: 9, 3: 6, 4: 14},\n",
       " 'jump': {2: 16},\n",
       " 'whole': {0: 4, 1: 1},\n",
       " 'unabl': {1: 1},\n",
       " 'occident': {4: 21},\n",
       " 'desk': {1: 2},\n",
       " 'ipsum': {3: 3},\n",
       " 'advantag': {5: 3},\n",
       " 'rung': {1: 2},\n",
       " 'etc': {4: 7},\n",
       " 'veldt': {2: 3},\n",
       " 'duti': {5: 6},\n",
       " 'blind': {3: 19, 5: 3},\n",
       " 'place': {1: 1, 3: 4},\n",
       " 'dull': {1: 2},\n",
       " 'lovabl': {2: 3},\n",
       " 'imag': {0: 3},\n",
       " 'tree': {0: 3},\n",
       " 'posit': {1: 2},\n",
       " 'discothequ': {2: 3},\n",
       " 'happi': {0: 3, 5: 3},\n",
       " 'or': {1: 2, 5: 17},\n",
       " 'greater': {0: 3, 5: 2},\n",
       " 'overspread': {0: 3},\n",
       " 'ocean': {3: 4},\n",
       " 'perfectli': {5: 3},\n",
       " 'wild': {3: 3},\n",
       " 'drop': {1: 1},\n",
       " 'an': {0: 3, 1: 1, 3: 3, 4: 7},\n",
       " 'adjust': {2: 3},\n",
       " 'most': {4: 7},\n",
       " 'than': {0: 3, 1: 2, 4: 14},\n",
       " 'given': {1: 1},\n",
       " 'gunboat': {2: 3},\n",
       " 'zipper': {2: 3},\n",
       " 'career': {1: 1},\n",
       " 'for': {0: 7, 1: 3, 2: 12, 3: 6, 4: 7},\n",
       " 'zompyc1': {2: 3},\n",
       " 'opal': {2: 3},\n",
       " 'bag': {2: 3},\n",
       " 'stupid': {1: 1},\n",
       " 'ill': {1: 4},\n",
       " 'recent': {1: 1},\n",
       " 'brawni': {2: 3},\n",
       " 'could': {0: 6, 1: 4, 3: 3, 4: 7},\n",
       " 'half': {1: 2},\n",
       " 'friendli': {1: 1},\n",
       " 'itself': {5: 6},\n",
       " 'luxuri': {1: 1},\n",
       " 'rais': {1: 1},\n",
       " 'flick': {2: 3},\n",
       " 'small': {1: 1, 3: 7},\n",
       " 'stroke': {0: 3},\n",
       " 'prevent': {5: 3},\n",
       " 'much': {0: 3, 1: 1},\n",
       " 'hill': {3: 3},\n",
       " 'isth': {4: 1},\n",
       " 'hand': {1: 1, 5: 3},\n",
       " 'overcom': {1: 1},\n",
       " 'jacket': {2: 3},\n",
       " 'sport': {4: 7},\n",
       " 'left': {3: 3},\n",
       " 'european': {4: 14},\n",
       " 'prog': {2: 3},\n",
       " 'occasion': {5: 3},\n",
       " 'text': {3: 19},\n",
       " 'grab': {2: 3},\n",
       " 'nor': {5: 3},\n",
       " 'box': {2: 3},\n",
       " 'look': {1: 4},\n",
       " 'parol': {3: 3},\n",
       " 'full': {0: 3},\n",
       " 'letter': {2: 3},\n",
       " 'zap': {2: 3},\n",
       " 'indign': {5: 3},\n",
       " 'weight': {0: 3},\n",
       " 'concept': {0: 3},\n",
       " 'foliag': {0: 3},\n",
       " 'up': {1: 8, 2: 6},\n",
       " 'armourlik': {1: 1},\n",
       " 'pack': {1: 1, 3: 3},\n",
       " 'consequ': {5: 6},\n",
       " 'great': {5: 6},\n",
       " 'funni': {1: 1},\n",
       " 'teach': {5: 3},\n",
       " 'familiar': {0: 3, 1: 1},\n",
       " 'grow': {0: 3},\n",
       " 'set': {1: 1},\n",
       " 'myself': {0: 3},\n",
       " 'incap': {0: 3},\n",
       " 'all': {0: 3, 1: 7, 2: 3, 5: 3},\n",
       " 'lower': {1: 1},\n",
       " 'your': {1: 3, 3: 3},\n",
       " 'painsbut': {5: 1},\n",
       " 'and': {0: 43, 1: 26, 2: 15, 3: 44, 4: 28, 5: 42},\n",
       " 'wouldnt': {1: 1},\n",
       " 'furniturerattl': {1: 1},\n",
       " 'particularli': {1: 1},\n",
       " 'flax': {2: 3},\n",
       " 'account': {5: 3},\n",
       " 'thin': {1: 1},\n",
       " 'absorb': {0: 6},\n",
       " 'bear': {0: 3},\n",
       " 'bosss': {1: 2},\n",
       " 'realiz': {4: 7},\n",
       " 'control': {3: 3},\n",
       " 'stray': {0: 3},\n",
       " 'oxmox': {3: 3},\n",
       " 'probabl': {1: 1},\n",
       " 'describ': {0: 3},\n",
       " 'we': {5: 6},\n",
       " 'tv': {2: 9},\n",
       " 'almighti': {0: 3},\n",
       " 'claim': {1: 1, 5: 3},\n",
       " 'taken': {0: 4},\n",
       " 'muff': {1: 1},\n",
       " 'spineless': {1: 1},\n",
       " 'while': {0: 3},\n",
       " 'best': {1: 1, 5: 3},\n",
       " 'collect': {1: 2},\n",
       " 'job': {2: 6},\n",
       " 'do': {1: 5, 3: 3, 5: 6},\n",
       " 'sever': {4: 7},\n",
       " 'born': {5: 3},\n",
       " 'more': {1: 4, 4: 21},\n",
       " 'over': {1: 1, 2: 7, 3: 3},\n",
       " 'come': {1: 1},\n",
       " 'sun': {0: 3},\n",
       " 'word': {3: 7, 4: 14},\n",
       " 'that': {0: 15, 1: 22, 3: 6, 4: 7, 5: 18},\n",
       " 'hi': {0: 3, 1: 20},\n",
       " 'labori': {5: 3},\n",
       " 'sleep': {1: 4},\n",
       " 'sphinx': {2: 3},\n",
       " 'red': {2: 3},\n",
       " 'touch': {1: 1},\n",
       " 'foxi': {2: 3},\n",
       " 'hell': {1: 1},\n",
       " 'ambush': {3: 3},\n",
       " 'me': {0: 9, 1: 2, 4: 7},\n",
       " 'gleam': {0: 3},\n",
       " 'within': {0: 3},\n",
       " 'waxth': {2: 1},\n",
       " 'man': {1: 1, 5: 6},\n",
       " 'road': {3: 3},\n",
       " 'bit': {1: 1},\n",
       " 'ration': {5: 3},\n",
       " 'cambridg': {4: 7},\n",
       " 'jumbl': {2: 3},\n",
       " 'point': {3: 3},\n",
       " 'seren': {0: 4},\n",
       " 'worri': {1: 1},\n",
       " 'vermin': {1: 1},\n",
       " 'gregor': {1: 5},\n",
       " 'watch': {2: 6},\n",
       " 'chang': {1: 1},\n",
       " 'sublin': {3: 3},\n",
       " 'idea': {5: 3},\n",
       " 'even': {1: 2, 3: 3},\n",
       " 'earth': {0: 6},\n",
       " 'buzz': {0: 3},\n",
       " 'common': {4: 28},\n",
       " 'first': {1: 1, 3: 3},\n",
       " 'duden': {3: 4},\n",
       " 'hung': {1: 1},\n",
       " 'no': {1: 1, 2: 3, 3: 3, 5: 9},\n",
       " 'extrem': {1: 1, 5: 3},\n",
       " 'pleasur': {5: 34},\n",
       " 'grace': {2: 3},\n",
       " 'if': {1: 6, 3: 3, 4: 7},\n",
       " 'herfar': {3: 1},\n",
       " 'draw': {0: 3},\n",
       " 'same': {4: 14, 5: 3},\n",
       " 'where': {1: 2, 3: 6},\n",
       " 'jumpi': {2: 3},\n",
       " 'slept': {1: 1},\n",
       " 'welcom': {5: 3},\n",
       " 'still': {1: 3, 3: 3},\n",
       " 'shudder': {1: 1},\n",
       " 'flock': {2: 6},\n",
       " 'etern': {0: 3},\n",
       " 'someth': {1: 1},\n",
       " 'zebra': {2: 6},\n",
       " 'her': {1: 1, 3: 41},\n",
       " 'individu': {4: 7},\n",
       " 'singl': {0: 3},\n",
       " 'creat': {0: 4},\n",
       " 'how': {1: 1, 2: 6, 5: 6},\n",
       " 'jodhpur': {2: 3},\n",
       " 'togeth': {1: 1},\n",
       " 'result': {4: 7, 5: 3},\n",
       " 'plaid': {2: 3},\n",
       " 'longer': {1: 1},\n",
       " 'kick': {1: 1},\n",
       " 'phoenix': {2: 3},\n",
       " 'strike': {0: 3},\n",
       " 'dwell': {0: 3},\n",
       " 'dome': {1: 1},\n",
       " 'notic': {0: 3, 1: 1},\n",
       " 'guest': {1: 1},\n",
       " 'woke': {1: 1},\n",
       " 'sampl': {1: 2},\n",
       " 'unorthograph': {3: 3},\n",
       " 'joke': {2: 3},\n",
       " 'complet': {5: 3},\n",
       " 'samsa': {1: 2},\n",
       " 'forc': {2: 3},\n",
       " 'english': {4: 14},\n",
       " 'piti': {1: 1},\n",
       " 'suppos': {1: 1},\n",
       " 'paper': {0: 3},\n",
       " 'god': {0: 3, 1: 2, 2: 3},\n",
       " 'flounder': {1: 1},\n",
       " 'fax': {2: 9},\n",
       " 'then': {0: 9, 1: 1, 3: 6},\n",
       " 'earthquak': {2: 3},\n",
       " 'entir': {0: 4, 1: 1},\n",
       " 'warn': {3: 3},\n",
       " 'physic': {5: 3},\n",
       " 'judg': {2: 3},\n",
       " 'mistaken': {5: 3},\n",
       " 'spread': {1: 1},\n",
       " 'everyon': {4: 7},\n",
       " 'way': {3: 9},\n",
       " 'wa': {0: 7, 1: 15, 2: 3, 3: 3, 5: 3},\n",
       " 'annoy': {5: 6},\n",
       " 'own': {0: 3, 1: 1, 3: 6},\n",
       " 'coast': {3: 4},\n",
       " 'new': {4: 14},\n",
       " 'transform': {1: 1},\n",
       " 'slowli': {1: 1},\n",
       " 'untrammel': {5: 3},\n",
       " 'between': {1: 1},\n",
       " 'quiver': {2: 3},\n",
       " 'slightli': {1: 1},\n",
       " 'girl': {2: 3},\n",
       " 'them': {1: 1},\n",
       " 'hed': {1: 1},\n",
       " 'quip': {2: 3},\n",
       " 'abl': {1: 1, 5: 3},\n",
       " 'tick': {1: 1},\n",
       " 'pig': {2: 3},\n",
       " 'there': {1: 9, 3: 7, 5: 3},\n",
       " 'fredericka': {2: 3},\n",
       " 'quartz': {2: 6},\n",
       " 'us': {0: 9, 5: 3},\n",
       " 'nice': {1: 1},\n",
       " 'alon': {0: 4},\n",
       " 'couldnt': {1: 1},\n",
       " 'game': {2: 3},\n",
       " 'hear': {0: 3, 1: 1},\n",
       " 'like': {0: 10, 1: 3, 4: 7, 5: 3},\n",
       " 'nymph': {2: 9},\n",
       " 'compar': {1: 1},\n",
       " 'free': {5: 3},\n",
       " 'day': {1: 2, 3: 3},\n",
       " 'heavi': {1: 1},\n",
       " 'wasnt': {1: 1},\n",
       " 'ye': {1: 1},\n",
       " 'line': {3: 6},\n",
       " 'enjoy': {0: 4, 5: 3},\n",
       " 'belov': {0: 3},\n",
       " 'exist': {0: 7, 4: 14},\n",
       " 'threw': {1: 1},\n",
       " 'earli': {1: 1},\n",
       " 'convinc': {3: 3},\n",
       " 'about': {1: 6, 3: 3},\n",
       " 'those': {5: 6},\n",
       " 'principl': {5: 3},\n",
       " 'accus': {1: 1},\n",
       " 'helplessli': {1: 1},\n",
       " 'salesman': {1: 1},\n",
       " 'demor': {5: 3},\n",
       " 'brick': {2: 3},\n",
       " 'lorem': {3: 3},\n",
       " 'chosen': {1: 1},\n",
       " 'dure': {1: 1},\n",
       " 'workshi': {1: 1},\n",
       " 'hard': {1: 2},\n",
       " 'yet': {0: 3, 1: 1},\n",
       " 'rewritten': {3: 6},\n",
       " 'parent': {1: 3},\n",
       " 'milk': {2: 3},\n",
       " 'regular': {4: 14},\n",
       " 'vision': {0: 2},\n",
       " 'pay': {1: 1, 4: 7},\n",
       " 'endur': {5: 2},\n",
       " 'eat': {1: 1},\n",
       " 'doctor': {1: 3},\n",
       " 'drunk': {3: 3},\n",
       " 'exercis': {5: 3},\n",
       " 'hit': {1: 1},\n",
       " 'juri': {2: 3},\n",
       " 'except': {5: 3},\n",
       " 'moment': {0: 3, 1: 1, 5: 3},\n",
       " 'sustain': {0: 3},\n",
       " 'told': {1: 1, 4: 7},\n",
       " 'pick': {2: 6},\n",
       " 'sanctuari': {0: 3},\n",
       " 'their': {1: 1, 3: 10, 4: 28, 5: 3},\n",
       " 'infinit': {0: 3},\n",
       " 'thing': {1: 1},\n",
       " 'well': {1: 1},\n",
       " 'origin': {3: 3},\n",
       " 'gentlemen': {1: 1},\n",
       " 'found': {1: 2},\n",
       " 'former': {1: 1},\n",
       " 'some': {1: 1, 5: 6},\n",
       " 'charm': {0: 4, 5: 3},\n",
       " 'accept': {1: 1, 5: 3},\n",
       " 'jack': {2: 6},\n",
       " 'viewer': {1: 1},\n",
       " 'thi': {0: 4, 1: 3, 4: 7, 5: 6},\n",
       " 'junk': {2: 6},\n",
       " 'countless': {0: 3},\n",
       " 'case': {1: 1, 5: 3},\n",
       " 'littl': {0: 3, 1: 4, 3: 9},\n",
       " 'project': {3: 3},\n",
       " 'enough': {1: 1},\n",
       " 'alarm': {1: 2},\n",
       " 'crazi': {2: 3},\n",
       " 'headlin': {3: 3},\n",
       " 'soon': {1: 1},\n",
       " 'mountain': {3: 7},\n",
       " 'which': {0: 11, 1: 2, 3: 3, 5: 9},\n",
       " 'abov': {1: 1},\n",
       " 'time': {1: 5, 3: 3},\n",
       " 'definit': {1: 1},\n",
       " 'bound': {5: 3},\n",
       " 'jug': {2: 3},\n",
       " 'herself': {3: 3},\n",
       " 'leg': {1: 3},\n",
       " 'surfac': {0: 3},\n",
       " 'jog': {2: 3},\n",
       " 'tell': {1: 1},\n",
       " 'too': {0: 3, 1: 1},\n",
       " 'owe': {5: 3},\n",
       " 'boss': {1: 4},\n",
       " 'push': {1: 1},\n",
       " 'forward': {1: 1},\n",
       " 'blue': {2: 3},\n",
       " 'drag': {3: 3},\n",
       " 'fact': {4: 7},\n",
       " 'sad': {1: 1},\n",
       " 'just': {1: 3, 2: 3},\n",
       " 'encount': {5: 3},\n",
       " 'is': {0: 9, 1: 2, 2: 3, 3: 6, 4: 19, 5: 18},\n",
       " 'mad': {1: 1},\n",
       " 'reach': {3: 3},\n",
       " 'zephyr': {2: 9},\n",
       " 'fault': {5: 3},\n",
       " 'ani': {1: 2, 5: 3},\n",
       " 'mild': {1: 1},\n",
       " 'rush': {1: 1},\n",
       " 'been': {1: 5, 3: 6},\n",
       " 'white': {1: 1},\n",
       " 'round': {1: 1},\n",
       " 'jukebox': {2: 3},\n",
       " 'dear': {0: 3},\n",
       " 'silk': {2: 3},\n",
       " 'versalia': {3: 3},\n",
       " 'hour': {5: 3},\n",
       " 'righteou': {5: 3},\n",
       " 'view': {3: 3},\n",
       " 'among': {0: 6},\n",
       " 'use': {1: 1, 3: 3, 4: 7},\n",
       " 'equal': {5: 3},\n",
       " 'spot': {0: 4, 1: 2},\n",
       " 'fit': {1: 1},\n",
       " 'put': {1: 1, 3: 3},\n",
       " 'iraq': {2: 3},\n",
       " 'matter': {5: 3},\n",
       " 'stalk': {0: 3},\n",
       " 'close': {0: 3, 1: 1},\n",
       " 'whelp': {2: 3},\n",
       " 'allpow': {3: 3},\n",
       " 'wall': {1: 1},\n",
       " 'let': {1: 1},\n",
       " 'dj': {2: 4},\n",
       " 'quart': {2: 3},\n",
       " 'advis': {3: 3},\n",
       " 'they': {3: 10, 5: 3},\n",
       " 'pain': {1: 1, 5: 30},\n",
       " 'bookmarksgrov': {3: 7},\n",
       " 'at': {0: 3, 1: 9, 3: 4},\n",
       " 'alway': {1: 2, 5: 3},\n",
       " 'give': {5: 3},\n",
       " 'abus': {3: 3},\n",
       " 'heaven': {0: 3, 1: 1},\n",
       " 'other': {1: 1, 5: 5},\n",
       " 'be': {0: 6, 1: 5, 3: 3, 4: 35, 5: 9},\n",
       " 'chump': {2: 3},\n",
       " 'tall': {0: 3},\n",
       " 'gild': {1: 1},\n",
       " 'in': {0: 16, 1: 9, 2: 6, 3: 7, 4: 14, 5: 15},\n",
       " 'who': {0: 3, 1: 2, 5: 21},\n",
       " 'ago': {1: 2},\n",
       " 'jig': {2: 3},\n",
       " 'stream': {0: 3},\n",
       " 'baz': {2: 3},\n",
       " 'cozi': {2: 3},\n",
       " 'uniform': {4: 7},\n",
       " 'flummox': {2: 3},\n",
       " 'pursu': {5: 6},\n",
       " 'blow': {2: 3},\n",
       " 'sexcharg': {2: 3},\n",
       " 'ought': {1: 1},\n",
       " 'dark': {0: 3},\n",
       " 'big': {1: 1, 2: 12, 3: 3},\n",
       " 'dozen': {2: 3},\n",
       " 'should': {0: 3, 1: 2, 3: 3}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(1,7):\n",
    "    with open(f\"./DataAssignment4/Text{i}.txt\") as f:\n",
    "        corpus.append(f.read())\n",
    "corpus = [preprocessing(doc) for doc in corpus]\n",
    "index = gen_idx(corpus)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', [0, 1, 3, 4, 5]),\n",
       " ('wa', [0, 1, 2, 3, 5]),\n",
       " ('be', [0, 1, 3, 4, 5]),\n",
       " ('a', [0, 1, 2, 3, 4, 5]),\n",
       " ('of', [0, 1, 2, 3, 4, 5]),\n",
       " ('the', [0, 1, 2, 3, 4, 5]),\n",
       " ('to', [0, 1, 2, 3, 4, 5]),\n",
       " ('and', [0, 1, 2, 3, 4, 5]),\n",
       " ('is', [0, 1, 2, 3, 4, 5]),\n",
       " ('in', [0, 1, 2, 3, 4, 5])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(k, list(v.keys())) for k, v in index.items()], key=lambda x: len(x[1]))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_raw_indexes(terms):\n",
    "    indexes = []\n",
    "    for term in terms:\n",
    "        if term in index:\n",
    "            indexes.append(index[term])\n",
    "        else:\n",
    "            indexes.append({})\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown', 'AND', ['fox', 'bear'], 'NOT', ['bubble', 'cat']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def parse_query(query):\n",
    "    # Tokenize the query\n",
    "    query = query.replace(\" -\", \" NOT \")\n",
    "    tokens = re.findall(r'(?:AND|OR|NOT|\\(|\\)|[a-zA-Z0-9]+)', query)\n",
    "    return parse_expression(tokens)\n",
    "\n",
    "def parse_expression(tokens):\n",
    "    current_expression = []\n",
    "\n",
    "    while tokens:\n",
    "        token = tokens.pop(0)\n",
    "\n",
    "        if token == \"(\":\n",
    "            # Start a new nested expression\n",
    "            nested_expression = parse_expression(tokens)\n",
    "            current_expression.append(nested_expression)\n",
    "\n",
    "        elif token == \")\":\n",
    "            # End the current expression\n",
    "            break\n",
    "        else:\n",
    "            current_expression.append(token)\n",
    "\n",
    "    return current_expression\n",
    "\n",
    "# Example usage:\n",
    "query = \"brown AND (fox bear) -(bubble cat)\"\n",
    "parsed_query = parse_query(query)\n",
    "parsed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_docs_by_intersection(terms):\n",
    "    # Get the indexes of the AND terms\n",
    "    indexes = retrieve_raw_indexes(terms)\n",
    "    indexes = sorted(indexes, key=len) # sort for efficiency\n",
    "    if not len(indexes):\n",
    "        return []\n",
    "    # Accumulate docs if they are in all other terms\n",
    "    docs = []\n",
    "    for key in indexes[0].keys(): # for all docs in first index\n",
    "        if all([key in idx.keys() for idx in indexes]): # the doc is in all other indexes\n",
    "            docs.append(key)\n",
    "    return docs\n",
    "id1 = [{2: 1, 3: 7}]\n",
    "query = [\"brown\", \"fox\"]\n",
    "get_docs_by_intersection([\"brown\", \"fox\", \"bear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_docs_by_union(terms):\n",
    "    # Get the indexes of the NOT terms\n",
    "    indexes = retrieve_raw_indexes(terms)\n",
    "    if not len(indexes):\n",
    "        return []\n",
    "    # Accumulate docs of the NOT terms\n",
    "    docs = set()\n",
    "    for index in indexes:\n",
    "        docs.update(index.keys())\n",
    "    return list(docs)\n",
    "\n",
    "get_docs_by_union([\"brown\", \"fox\", \"bear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(term):\n",
    "    term = preprocessing(term)\n",
    "    if term in index:\n",
    "        return set(index[term].keys())\n",
    "    return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_recursive(query, doc_set:set=set()):\n",
    "    current_context = set()\n",
    "    operator = None\n",
    "\n",
    "    for term in query:\n",
    "        if isinstance(term, list):\n",
    "            # Recursively process sub-expression\n",
    "            sub_result = search_recursive(term, doc_set)\n",
    "            if operator == 'AND':\n",
    "                doc_set.intersection_update(sub_result)\n",
    "            elif operator == 'NOT':\n",
    "                doc_set.difference_update(sub_result)\n",
    "            else: # default OR\n",
    "                doc_set.update(sub_result)\n",
    "        elif term in {'AND', 'OR', 'NOT', '-'}:\n",
    "            # Set the current operator\n",
    "            operator = term\n",
    "        else:\n",
    "            # Process term\n",
    "            term_docs = retrieve_docs(term)  # Replace with your actual retrieval function\n",
    "            if operator == 'AND':\n",
    "                if not current_context:\n",
    "                    current_context = term_docs\n",
    "                else:\n",
    "                    current_context.intersection_update(term_docs)\n",
    "            elif operator == 'OR':\n",
    "                current_context.update(term_docs)\n",
    "            elif operator == 'NOT':\n",
    "                current_context.difference_update(term_docs)\n",
    "            else:\n",
    "                current_context.update(term_docs)\n",
    "    \n",
    "    # After processing all terms and operators in the query, update the document set\n",
    "    doc_set.update(current_context)\n",
    "    \n",
    "    return doc_set\n",
    "\n",
    "\n",
    "def search(query):\n",
    "    parsed_query = parse_query(query)\n",
    "    return list(search_recursive(parsed_query))\n",
    "\n",
    "# query = \"brown AND (fox) -(bubble cat)\"\n",
    "# query = \"enjoy AND bears\"\n",
    "# docs = search(query)\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516, [1, 5], ['claim', 'and'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"(claim AND banan)\"\n",
    "docs = search(query)\n",
    "\n",
    "docs_full = [corpus[i] for i in docs]\n",
    "docs_full = [preprocessing(doc) for doc in docs_full]\n",
    "vocab = set(\" \".join(docs_full).split(\" \"))\n",
    "\n",
    "query = preprocessing(query).split(\" \")\n",
    "query = list(filter(lambda x: x in vocab, query))\n",
    "len(vocab), docs, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 26]\n",
      "[3, 42]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print([index[term][doc] if doc in index[term] else 0 for term in query])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 5, Score: 4.7549, Abstract: but i must explain to you how all thi mistaken idea of denounc pleasur and prais pain wa born and i will give you a complet account of the system and expound the actual teach of the great explor of the truth the masterbuild of human happi no one reject dislik or avoid pleasur itself becaus it is pleasur but becaus those who do not know how to pursu pleasur ration encount consequ that are extrem pain nor again is there anyon who love or pursu or desir to obtain pain of itself becaus it is pain but becaus occasion circumst occur in which toil and pain can procur him some great pleasur to take a trivial exampl which of us ever undertak labori physic exercis except to obtain some advantag from it but who ha ani right to find fault with a man who choos to enjoy a pleasur that ha no annoy consequ or one who avoid a pain that produc no result pleasur on the other hand we denounc with righteou indign and dislik men who are so beguil and demor by the charm of pleasur of the moment so blind by desir that they cannot forese the pain and troubl that are bound to ensu and equal blame belong to those who fail in their duti through weak of will which is the same as say through shrink from toil and pain these case are perfectli simpl and easi to distinguish in a free hour when our power of choic is untrammel and when noth prevent our be abl to do what we like best everi pleasur is to be welcom and everi pain avoid but in certain circumst and owe to the claim of duti or the oblig of busi it will frequent occur that pleasur have to be repudi and annoy accept the wise man therefor alway hold in these matter to thi principl of select he reject pleasur to secur other greater pleasur or els he endur pain to avoid wors pain but i must explain to you how all thi mistaken idea of denounc pleasur and prais pain wa born and i will give you a complet account of the system and expound the actual teach of the great explor of the truth the masterbuild of human happi no one reject dislik or avoid pleasur itself becaus it is pleasur but becaus those who do not know how to pursu pleasur ration encount consequ that are extrem pain nor again is there anyon who love or pursu or desir to obtain pain of itself becaus it is pain but becaus occasion circumst occur in which toil and pain can procur him some great pleasur to take a trivial exampl which of us ever undertak labori physic exercis except to obtain some advantag from it but who ha ani right to find fault with a man who choos to enjoy a pleasur that ha no annoy consequ or one who avoid a pain that produc no result pleasur on the other hand we denounc with righteou indign and dislik men who are so beguil and demor by the charm of pleasur of the moment so blind by desir that they cannot forese the pain and troubl that are bound to ensu and equal blame belong to those who fail in their duti through weak of will which is the same as say through shrink from toil and pain these case are perfectli simpl and easi to distinguish in a free hour when our power of choic is untrammel and when noth prevent our be abl to do what we like best everi pleasur is to be welcom and everi pain avoid but in certain circumst and owe to the claim of duti or the oblig of busi it will frequent occur that pleasur have to be repudi and annoy accept the wise man therefor alway hold in these matter to thi principl of select he reject pleasur to secur other greater pleasur or els he endur pain to avoid wors painsbut i must explain to you how all thi mistaken idea of denounc pleasur and prais pain wa born and i will give you a complet account of the system and expound the actual teach of the great explor of the truth the masterbuild of human happi no one reject dislik or avoid pleasur itself becaus it is pleasur but becaus those who do not know how to pursu pleasur ration encount consequ that are extrem pain nor again is there anyon who love or pursu or desir to obtain pain of itself becaus it is pain but becaus occasion circumst occur in which toil and pain can procur him some great pleasur to take a trivial exampl which of us ever undertak labori physic exercis except to obtain some advantag from it but who ha ani right to find fault with a man who choos to enjoy a pleasur that ha no annoy consequ or one who avoid a pain that produc no result pleasur on the other hand we denounc with righteou indign and dislik men who are so beguil and demor by the charm of pleasur of the moment so blind by desir that they cannot forese the pain and troubl that are bound to ensu and equal blame belong to those who fail in their duti through weak of will which is the same as say through shrink from toil and pain these case are perfectli simpl and easi to distinguish in a free hour when our power of choic is untrammel and when noth prevent our be abl to do what we like best everi pleasur is to be welcom and everi pain avoid but in certain circumst and owe to the claim of duti or the oblig of busi it will frequent occur that pleasur have to be repudi and annoy accept the wise man therefor alway hold in these matter to thi principl of select\n",
      "ID: 1, Score: 1.5850, Abstract: one morn when gregor samsa woke from troubl dream he found himself transform in hi bed into a horribl vermin he lay on hi armourlik back and if he lift hi head a littl he could see hi brown belli slightli dome and divid by arch into stiff section the bed wa hardli abl to cover it and seem readi to slide off ani moment hi mani leg piti thin compar with the size of the rest of him wave about helplessli as he look what happen to me he thought it wasnt a dream hi room a proper human room although a littl too small lay peac between it four familiar wall a collect of textil sampl lay spread out on the tabl samsa wa a travel salesman and abov it there hung a pictur that he had recent cut out of an illustr magazin and hous in a nice gild frame it show a ladi fit out with a fur hat and fur boa who sat upright rais a heavi fur muff that cover the whole of her lower arm toward the viewer gregor then turn to look out the window at the dull weather drop of rain could be heard hit the pane which made him feel quit sad how about if i sleep a littl bit longer and forget all thi nonsens he thought but that wa someth he wa unabl to do becaus he wa use to sleep on hi right and in hi present state couldnt get into that posit howev hard he threw himself onto hi right he alway roll back to where he wa he must have tri it a hundr time shut hi eye so that he wouldnt have to look at the flounder leg and onli stop when he began to feel a mild dull pain there that he had never felt befor oh god he thought what a strenuou career it is that ive chosen travel day in and day out do busi like thi take much more effort than do your own busi at home and on top of that there the curs of travel worri about make train connect bad and irregular food contact with differ peopl all the time so that you can never get to know anyon or becom friendli with them it can all go to hell he felt a slight itch up on hi belli push himself slowli up on hi back toward the headboard so that he could lift hi head better found where the itch wa and saw that it wa cover with lot of littl white spot which he didnt know what to make of and when he tri to feel the place with one of hi leg he drew it quickli back becaus as soon as he touch it he wa overcom by a cold shudder he slid back into hi former posit get up earli all the time he thought it make you stupid youv got to get enough sleep other travel salesmen live a life of luxuri for instanc whenev i go back to the guest hous dure the morn to copi out the contract these gentlemen are alway still sit there eat their breakfast i ought to just tri that with my boss id get kick out on the spot but who know mayb that would be the best thing for me if i didnt have my parent to think about id have given in my notic a long time ago id have gone up to the boss and told him just what i think tell him everyth i would let him know just what i feel hed fall right off hi desk and it a funni sort of busi to be sit up there at your desk talk down at your subordin from up there especi when you have to go right up close becaus the boss is hard of hear well there still some hope onc ive got the money togeth to pay off my parent debt to him anoth five or six year i suppos that definit what ill do that when ill make the big chang first of all though ive got to get up my train leav at five and he look over at the alarm clock tick on the chest of drawer god in heaven he thought it wa half past six and the hand were quietli move forward it wa even later than half past more like quarter to seven had the alarm clock not rung he could see from the bed that it had been set for four oclock as it should have been it certainli must have rung ye but wa it possibl to quietli sleep through that furniturerattl nois true he had not slept peac but probabl all the more deepli becaus of that what should he do now the next train went at seven if he were to catch that he would have to rush like mad and the collect of sampl wa still not pack and he did not at all feel particularli fresh and live and even if he did catch the train he would not avoid hi bosss anger as the offic assist would have been there to see the five oclock train go he would have put in hi report about gregor not be there a long time ago the offic assist wa the bosss man spineless and with no understand what about if he report sick but that would be extrem strain and suspici as in fifteen year of servic gregor had never onc yet been ill hi boss would certainli come round with the doctor from the medic insur compani accus hi parent of have a lazi son and accept the doctor recommend not to make ani claim as the doctor believ that noon wa ever ill but that mani were workshi and what more would he have been entir wrong in thi case gregor\n"
     ]
    }
   ],
   "source": [
    "tfs = np.array([[(index[term][doc] if doc in index[term] else 0) for term in query] for doc in docs])\n",
    "dfs = np.array([len(index[term]) for term in query])\n",
    "idfs = np.log2(len(corpus) / dfs)\n",
    "tf_idf = np.sum(tfs*idfs, axis=1)\n",
    "scores_docs = {doc: score for doc, score in zip(docs, tf_idf)}\n",
    "sorted_docs = dict(sorted(scores_docs.items(), key=lambda x: x[1])[::-1])\n",
    "for k, v in sorted_docs.items():\n",
    "    print(f\"ID: {k}, Score: {v:.4f}, Abstract: {corpus[k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\OneDrive\\uni\\7.semester\\Informasjonsgjenfinning\\assignments\\assignment-4\\indexer.ipynb Cell 29\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/uni/7.semester/Informasjonsgjenfinning/assignments/assignment-4/indexer.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[index[term][doc] \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m vocab] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/uni/7.semester/Informasjonsgjenfinning/assignments/assignment-4/indexer.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mlen\u001b[39m(index[term]) \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m vocab])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/uni/7.semester/Informasjonsgjenfinning/assignments/assignment-4/indexer.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m idfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog2(\u001b[39mlen\u001b[39m(corpus) \u001b[39m/\u001b[39m dfs)\n",
      "\u001b[1;32mc:\\Users\\andre\\OneDrive\\uni\\7.semester\\Informasjonsgjenfinning\\assignments\\assignment-4\\indexer.ipynb Cell 29\u001b[0m line \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/uni/7.semester/Informasjonsgjenfinning/assignments/assignment-4/indexer.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[index[term][doc] \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m vocab] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/uni/7.semester/Informasjonsgjenfinning/assignments/assignment-4/indexer.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mlen\u001b[39m(index[term]) \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m vocab])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/uni/7.semester/Informasjonsgjenfinning/assignments/assignment-4/indexer.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m idfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog2(\u001b[39mlen\u001b[39m(corpus) \u001b[39m/\u001b[39m dfs)\n",
      "\u001b[1;32mc:\\Users\\andre\\OneDrive\\uni\\7.semester\\Informasjonsgjenfinning\\assignments\\assignment-4\\indexer.ipynb Cell 29\u001b[0m line \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/uni/7.semester/Informasjonsgjenfinning/assignments/assignment-4/indexer.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[index[term][doc] \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m vocab] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/uni/7.semester/Informasjonsgjenfinning/assignments/assignment-4/indexer.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mlen\u001b[39m(index[term]) \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m vocab])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/uni/7.semester/Informasjonsgjenfinning/assignments/assignment-4/indexer.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m idfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog2(\u001b[39mlen\u001b[39m(corpus) \u001b[39m/\u001b[39m dfs)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "tfs = np.array([[index[term][doc] for term in vocab] for doc in docs])\n",
    "dfs = np.array([len(index[term]) for term in vocab])\n",
    "idfs = np.log2(len(corpus) / dfs)\n",
    "\n",
    "tf_idf = tfs*idfs\n",
    "\n",
    "scores_docs = zip(tf_idf, docs)\n",
    "\n",
    "sorted_docs = sorted(scores_docs, key=lambda x: x[0])[::-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
